<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Building an Orchid Genus Classifier App Using fastai, Render and Flutter - Part 2 | Mike’s Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Building an Orchid Genus Classifier App Using fastai, Render and Flutter - Part 2" />
<meta name="author" content="Mike Fuller" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A blog about my journey into machine learning and AI" />
<meta property="og:description" content="A blog about my journey into machine learning and AI" />
<link rel="canonical" href="https://mikful.github.io/blog/fastai/jupyter/render/flutter/2020/09/16/orchid-classifier-fastai-render-flutter-2.html" />
<meta property="og:url" content="https://mikful.github.io/blog/fastai/jupyter/render/flutter/2020/09/16/orchid-classifier-fastai-render-flutter-2.html" />
<meta property="og:site_name" content="Mike’s Blog" />
<meta property="og:image" content="https://mikful.github.io/blog/images/orchid-classifier/test-odontoglossum-crispum-centred.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-16T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Building an Orchid Genus Classifier App Using fastai, Render and Flutter - Part 2","dateModified":"2020-09-16T00:00:00-05:00","url":"https://mikful.github.io/blog/fastai/jupyter/render/flutter/2020/09/16/orchid-classifier-fastai-render-flutter-2.html","datePublished":"2020-09-16T00:00:00-05:00","@type":"BlogPosting","image":"https://mikful.github.io/blog/images/orchid-classifier/test-odontoglossum-crispum-centred.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"https://mikful.github.io/blog/fastai/jupyter/render/flutter/2020/09/16/orchid-classifier-fastai-render-flutter-2.html"},"author":{"@type":"Person","name":"Mike Fuller"},"description":"A blog about my journey into machine learning and AI","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mikful.github.io/blog/feed.xml" title="Mike's Blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Mike&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Building an Orchid Genus Classifier App Using fastai, Render and Flutter - Part 2</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-16T00:00:00-05:00" itemprop="datePublished">
        Sep 16, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Mike Fuller</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#jupyter">jupyter</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#render">render</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#flutter">flutter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/mikful/mike-blog/tree/master/_notebooks/2020-09-16-orchid-classifier-fastai-render-flutter-2.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/mikful/mike-blog/master?filepath=_notebooks%2F2020-09-16-orchid-classifier-fastai-render-flutter-2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/mikful/mike-blog/blob/master/_notebooks/2020-09-16-orchid-classifier-fastai-render-flutter-2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-16-orchid-classifier-fastai-render-flutter-2.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this blog series I'll show how to train an orchid genus classifier using <a href="https://www.fast.ai/">fastai</a>, deploy this to <a href="https://render.com/">Render</a> and create a Flutter app for the front-end. This will be done in two parts:</p>
<ol>
<li><a href="https://mikful.github.io/blog/fastai/jupyter/render/flutter/2020/09/15/orchid-classifier-fastai-render-flutter-1.html">Dataset Collection and fastai Image Classifier Training</a></li>
<li>Render Deployment and Flutter App</li>
</ol>
<p>This is part 2, please see the associated <a href="https://github.com/mikful/orchid-classifier-render">github repo for render</a> and <a href="https://github.com/mikful/orchid-classifier-flutter">github repo for flutter</a> for further implementation details.</p>
<h2 id="Render-Deployment">Render Deployment<a class="anchor-link" href="#Render-Deployment"> </a></h2><p>Now that we've trained our model, we need to deploy our fastai Learner to a dockerized environment, such that we can perform inference on new images. The fastai course v3 had a <a href="https://github.com/render-examples/fastai-v3">starter package</a> for Render that we'll update to work with the latest fastai library (version 2) and also for fastapi in this case. (Note: the fastbook and fastai website now give different options, such as a binder-hosted jupyter notebook for simple inference, or <a href="https://course.fast.ai/deployment_seeme_ai">seemeai</a>, although I find render and heroku are still very good options and more flexible than binder.)</p>
<h3 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"> </a></h3><p>First, let's update our package dependencies (that we listed within our Notebook environment in Part 1 of this blog series). Be sure to use the CPU versions of PyTorch versions within the Notebook/Colab Environment for the docker deployment, not the larger Cuda enabled wheels as for the inference we do not require GPU usage. You can ensure to set these using <code>+cpu</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">==</span><span class="mf">1.6</span><span class="o">.</span><span class="mi">0</span><span class="o">+</span><span class="n">cpu</span> 
<span class="n">torchvision</span><span class="o">==</span><span class="mf">0.7</span><span class="o">.</span><span class="mi">0</span><span class="o">+</span><span class="n">cpu</span>
<span class="o">-</span><span class="n">f</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">download</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">whl</span><span class="o">/</span><span class="n">torch_stable</span><span class="o">.</span><span class="n">html</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These and all other dependencies in the requirements.txt file will be installed when the Dockerfile builds the container image using the following line:</p>
<p><code>RUN pip install --upgrade -r requirements.txt</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="server.py">server.py<a class="anchor-link" href="#server.py"> </a></h3><p>Now we need to update the server.py file that contains the app. Firstly, the original <a href="https://www.starlette.io/">Starlette</a> asyncio library was replaced by the <a href="https://fastapi.tiangolo.com/">fastapi</a> library, which is a little clearer in its syntax and easier to use.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we define the location of our exported fastai Learner. In part 1, we saw how to upload this to a Google Cloud Storage bucket - we need to ensure the permissions on the file are set such that we can download this, which can be done in the file settings in the bucket (for more details <a href="https://cloud.google.com/storage/docs/access-control/making-data-public">see here</a>).</p>
<p>Then we set this within the script:</p>
<div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
<span class="n">export_file_url</span> <span class="o">=</span> <span class="s1">&#39;https://storage.googleapis.com/fastai-export-bucket/export.pkl&#39;</span> <span class="c1"># google cloud bucket / file url</span>
<span class="n">export_file_name</span> <span class="o">=</span> <span class="s1">&#39;export.pkl&#39;</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Inference</strong></p>
<p>The main functions within the server.py file are fairly self evident, in that it downloads the <code>export.pkl</code> file and then loads it into a new fastai <code>Learner</code>.</p>
<p>For the inference, the main function is contained within the following code section that takes the image bytes from the post request and performs the inference on it. The prediction is then returned in the JSON response.</p>
<div class="highlight"><pre><span></span><span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/analyze&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">analyze</span><span class="p">(</span><span class="n">file</span><span class="p">:</span> <span class="nb">bytes</span> <span class="o">=</span> <span class="n">File</span><span class="p">(</span><span class="o">...</span><span class="p">)):</span>
    <span class="n">img_bytes</span> <span class="o">=</span> <span class="n">BytesIO</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="n">prediction</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img_bytes</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">JSONResponse</span><span class="p">({</span><span class="s1">&#39;result&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">prediction</span><span class="p">)})</span>
</pre></div>
<p><strong>HTML</strong></p>
<p>Note that within the repo is also the css and <a href="https://github.com/mikful/orchid-classifier-render/blob/master/app/view/index.html">html file</a> for a webapp, that can be used as the interface to perform the inference if desired, instead of the Flutter app. However, in this case, I wanted to make a full Android app, not a webapp.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Flutter-App">Flutter App<a class="anchor-link" href="#Flutter-App"> </a></h2><p>Flutter is a cross-platform app development solution, with near native application speeds.</p>
<p>Thanks to the code given at: <a href="https://github.com/dnmanveet/Fruit_classifier_app">https://github.com/dnmanveet/Fruit_classifier_app</a> I could successfully connect my Render backend to a working Android Flutter App.</p>
<p>It was really as simple as changing the base render deployment location within <a href="https://github.com/mikful/orchid-classifier-flutter/blob/6fa185dd2a61e5911d67be69ee6c38c8d46c8ba7/orchid-classifier-flutter/lib/main.dart">main.dart</a>.</p>
<div class="highlight"><pre><span></span><span class="n">String</span> <span class="n">base</span> <span class="o">=</span>
        <span class="s2">&quot;https://orchid-classifier.onrender.com&quot;</span><span class="p">;</span>
</pre></div>
<p>Then, all that remained were some interface and styling tweaks and I had a pretty decent looking basic app for trying out on my own images!</p>
<p><p>&nbsp;</p>
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/../images/orchid-classifier/app.jpg" alt="" style="max-width: 300px" />
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Testing">Testing<a class="anchor-link" href="#Testing"> </a></h2><p>Interestingly, in testing the predictions were initially way off, with many errors, however, I decided to try and zoom in on the images to see if the problem was related to the angle and distance of the photos. This resulted in a vast improvement - the model is clearly very sensitive to distance of the photo, or size of the flower relative to the image. This perhaps is unsurprising given that many of the images in the training data are professionally shot, resulting in well-centred and close-up images of the flowers in many cases, thereby containg much more information regarding the flower structure than an image shot further away, with little definition between the flowers.</p>
<p><p>&nbsp;</p></p>
<figure>
    <div style="display:flex">
        <div style="flex:1">
            <figure>
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/../images/orchid-classifier/cymbidium.jpg" alt="" style="max-width: 300px" />
    
    
</figure>

                <figcaption><center>Original photo = incorrect prediction</center></figcaption>
            </figure>
        </div>
        <div style="flex:1">
            <figure>
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/../images/orchid-classifier/cymbidium-zoomed.jpg" alt="" style="max-width: 300px" />
    
    
</figure>

                <figcaption><center>Zoomed photo = correct prediction</center></figcaption>
            </figure>
        </div>
    </div>
</figure><p><p>&nbsp;</p></p>
<figure>
    <div style="display:flex">
        <div style="flex:1">
            <figure>
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/../images/orchid-classifier/odont1.jpg" alt="" style="max-width: 300px" />
    
    
</figure>

                <figcaption><center>Original photo = incorrect prediction</center></figcaption>
            </figure>
        </div>
        <div style="flex:1">
            <figure>
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/../images/orchid-classifier/odont1-zoomed.jpg" alt="" style="max-width: 300px" />
    
    
</figure>

                <figcaption><center>Zoomed photo = correct prediction</center></figcaption>
            </figure>
        </div>
    </div>
</figure><p><p>&nbsp;</p></p>
<figure>
    <div style="display:flex">
        <div style="flex:1">
            <figure>
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/../images/orchid-classifier/odont2.jpg" alt="" style="max-width: 300px" />
    
    
</figure>

                <figcaption><center>Original photo = incorrect prediction</center></figcaption>
            </figure>
        </div>
        <div style="flex:1">
            <figure>
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/../images/orchid-classifier/odont2-zoomed.jpg" alt="" style="max-width: 300px" />
    
    
</figure>

                <figcaption><center>Zoomed photo = correct prediction</center></figcaption>
            </figure>
        </div>
    </div>
</figure>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>&nbsp;</p><p>With photos taken face-on and focusing on the flowers the results were immediately of much higher accuracy:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/../images/orchid-classifier/masdevallia.jpg" alt="" style="max-width: 300px" />
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>&nbsp;</p><p>As such, it will be necessary to build in a zoom function for cropping and centering the flower head before inference. In addition, on some rarer varieties of orchid species I tried there was a clear error, as the rarer species may not have been covered in the test data, however, I'm surprised at how well this works in general and will be useful practical tool (with a bit of verification of course!).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2><p>All in all, this was a fun little project that appears to be giving quite decent results in the real world. Improvements and next steps for the app development, would be the following:</p>
<ul>
<li>A zoom function to centre the flower head before the inference is made</li>
<li>Create an S3 storage bucket to store all the uploaded photos for further training of the model</li>
<li>Create an option for the user to confirm the prediction as correct or incorrect, and input the real flower genus if so, again for further training</li>
</ul>
<p>Another promising approach may be to use an architecture for more fine-grained feature recognition, such as the NTS-net described in the paper <a href="https://arxiv.org/abs/1809.00287">Learning to Navigate for Fine-grained Classification</a> or a multi-modal network, that contains both descriptive text information and images. In addition, perhaps dimensional measurements would be useful for the classificaton process and would help to account for the very subtle differences between the genera.</p>
<p>I may update this blog series with a part 3 at another time including these items if I can incorporate them.</p>
<p>I hope this series may be of use to someone, and if anyone reading this has any queries regarding the training or deployment, or any comments in general, please let me know.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="mikful/mike-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/fastai/jupyter/render/flutter/2020/09/16/orchid-classifier-fastai-render-flutter-2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog about my journey into machine learning and AI</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/mikful" title="mikful"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/mikful" title="mikful"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
