{
  
    
        "post0": {
            "title": "Audio Classification Using Lightweight 1D End-to-End CNNs",
            "content": "In this notebook we&#39;ll be investigating audio classification on the ESC-10 dataset using very lightweight 1D end-to-end CNN architectures that learn features directly from the waveform, using the PyTorch torchaudio library and PyTorch Lightning. . The architectures were first described by Abdoli et al., 2019, within the research paper &quot;End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network&quot; (https://arxiv.org/pdf/1904.08990.pdf). . These architectures are interesting from a few standpoints. Firstly, they are extremely lightweight, with the largest having ~550,000 parameters, as compared to, for example, a ResNet18 2D architecture, which has approximately 11 million parameters. The smallest network described has only 256,538 parameters, which is a 36x size reduction from the ResNet18. This therefore makes them highly suited to resource-constrained or embedded situations, whereby on-device inference with small compute may be a requirement and a larger 2D architecture that uses mel-spectrograms may not be suitable. . In addition, the architectures have a novel structure, whereby the first layers have a very wide receptive field, such that they can take a whole second or two of data at a 16kHz sampling rate with a large initial kernel-filter size and the later layers have smaller receptive fields after successive convolutions. This in theory allows the early layers to learn more general features representative of the sound directly from the waveform, with the later layers learning more fine-grained features, all within a very compact form. . In addition, this blog demonstrates the use of a custom collate_fn batching function, which allows the 5-second long audio filed to be split into overlapping windowed sub-frames, in order to allow the majority sum rule to be applied to the predictions on each sub-frame to give the overall prediction per example. . Three network architectures will be compared: . 16k = 1 seconds of input at 16kHz (i.e. a single channel 16,000 dimension input, 9x sub-frames with 0.5s overlap) | 32k = 2 seconds of input at 16kHz (i.e. a single channel 32,000 dimension input, 5x sub-frames with 0.75s overlap) | 16k Gammatone filterbank initialised network (the same input as 16k), using the asteroid library | . This will be done using 5-fold cross-validation accuracy. . Note: the following notebook was undertaken within Paperspace Gradient. To use within Colab the datasets must first be downloaded and extracted and the paths defined. . import torch from torch import nn, optim, Tensor from torchvision import datasets, transformsz from torch.utils.data import Dataset, DataLoader import torch.nn.functional as F import torchaudio import pytorch_lightning as pl from pytorch_lightning.metrics import Accuracy from pytorch_lightning.loggers.wandb import WandbLogger import wandb from tqdm import tqdm import os from pathlib import Path import pandas as pd from matplotlib import pyplot as plt import random import IPython.display as ipd . Define dataset . path = Path(&#39;/storage/mydatasets/ESC-50&#39;) . df = pd.read_csv(path/&quot;meta&quot;/&quot;esc50.csv&quot;) df.head() . filename fold target category esc10 src_file take . 0 1-100032-A-0.wav | 1 | 0 | dog | True | 100032 | A | . 1 1-100038-A-14.wav | 1 | 14 | chirping_birds | False | 100038 | A | . 2 1-100210-A-36.wav | 1 | 36 | vacuum_cleaner | False | 100210 | A | . 3 1-100210-B-36.wav | 1 | 36 | vacuum_cleaner | False | 100210 | B | . 4 1-101296-A-19.wav | 1 | 19 | thunderstorm | False | 101296 | A | . Let&#39;s have a listen to an example from our dataset: . wavs = list(path.glob(&#39;audio/*&#39;)) #load to tensor for viewing waveform, sample_rate = torchaudio.load(wavs[0]) print(&quot;Shape of waveform: {}&quot;.format(waveform.size())) print(&quot;Sample rate of waveform: {}&quot;.format(sample_rate)) plt.figure() plt.plot(waveform.t().numpy()) ipd.Audio(waveform, rate=sample_rate) # load tensor from dataset . Shape of waveform: torch.Size([1, 220500]) Sample rate of waveform: 44100 . Your browser does not support the audio element. Dataset . Now we can create our dataset, by loading and normalizing then files, then resampling to 16kHz and splitting them into successive overlapped sub-frames using the unfold PyTorch method: . class ESCData(Dataset): def __init__(self, dataset, **kwargs): self.data_dir = kwargs[&quot;data_dir&quot;] self.df = kwargs[&quot;df&quot;] self.valid_fold = kwargs[&quot;valid_fold&quot;] self.test_fold = kwargs[&quot;test_fold&quot;] self.esc10 = kwargs[&quot;esc10&quot;] self.file_col = kwargs[&quot;file_col&quot;] self.label_col = kwargs[&quot;label_col&quot;] self.sr = kwargs[&quot;sr&quot;] self.new_sr = kwargs[&quot;new_sr&quot;] self.sample_len_s= kwargs[&quot;sample_len_s&quot;] if self.esc10: self.df = df.loc[df[&#39;esc10&#39;]==True] if dataset == &quot;train&quot;: self.df = self.df.loc[(self.df[&#39;fold&#39;] != self.valid_fold) &amp; (self.df[&#39;fold&#39;] != self.test_fold)] # create new df based on fold for train split elif dataset == &quot;val&quot;: self.df = self.df.loc[self.df[&#39;fold&#39;] == self.valid_fold] # create new df based on fold for valid split elif dataset == &quot;test&quot;: self.df = self.df.loc[self.df[&#39;fold&#39;] == self.test_fold] # create new df based on fold for valid split self.categories = sorted(self.df[self.label_col].unique()) #initialize lists to hold file names, labels, and folder numbers self.file_names = [] self.labels = [] self.categories = sorted(self.df[self.label_col].unique()) self.c2i={} self.i2c={} self.categories = sorted(self.df[self.label_col].unique()) for i, category in enumerate(self.categories): self.c2i[category]=i self.i2c[i]=category for ind in tqdm(range(len(self.df))): row = self.df.iloc[ind] file_path = path / &quot;audio&quot; / row[self.file_col] self.file_names.append(file_path) self.labels.append(self.c2i[row[self.label_col]]) self.resample = torchaudio.transforms.Resample(self.sr, self.new_sr) # resample to chosen rate # window size for rolling window sample splits (unfold method) if self.sample_len_s == 2: self.window_size = self.new_sr * 2 self.step_size = int(self.new_sr*0.75) else: self.window_size = self.new_sr self.step_size = int(self.new_sr*0.5) def __getitem__(self, index): # split audio files into 1s@16kHz or 2s@32kHz tensors with 0.5s overlap # pass as stacked tensors tensor with single label path = self.file_names[index] audio_file = torchaudio.load(path, out=None, normalization=True) # load the normalized file to tensor audioTensor = self.resample(audio_file[0]) # resample splits = audioTensor.unfold(1, self.window_size, self.step_size) # window_size set in attributes, step size of 0.5s samples = splits.permute(1,0,2) # permute to shape (n-samples, channel, sample len) return samples, self.labels[index] def __len__(self): return len(self.file_names) . Now, we create our LightningDataModule to give us our Dataset and Dataloaders for the training and validation sets. . class DataModule(pl.LightningDataModule): def __init__(self, **kwargs): super().__init__() self.batch_size = kwargs[&quot;batch_size&quot;] self.num_workers = kwargs[&quot;num_workers&quot;] self.kwargs = kwargs def setup(self, stage = None): # Define our Datasets if stage == &#39;fit&#39; or stage is None: self.data_train = ESCData(dataset=&quot;train&quot;, **self.kwargs) self.data_val = ESCData(dataset=&quot;val&quot;, **self.kwargs) if stage == &#39;test&#39; or stage is None: self.data_test = ESCData(dataset=&quot;test&quot;, **self.kwargs ) def train_dataloader(self): return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=True, collate_fn = self.collate_fn, num_workers=self.num_workers) def val_dataloader(self): return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=False, collate_fn = self.collate_fn, num_workers=self.num_workers) def test_dataloader(self): return DataLoader(self.data_test, batch_size=32, shuffle=False, collate_fn = self.collate_fn, num_workers=self.num_workers) def collate_fn(self, data): &quot;&quot;&quot; data: is a tuple of 2 tuples with (example, label) where example are the split 1 second sub-frame audio tensors per file label = the label &quot;&quot;&quot; examples, labels = zip(*data) examples = torch.cat(examples) labels = torch.flatten(torch.tensor(labels)) return [examples, labels] . data = DataModule(batch_size=99, num_workers=8, data_dir=path, df=df, valid_fold=1, test_fold=0, # set to 0 for no test set esc10=True, file_col=&#39;filename&#39;, label_col=&#39;category&#39;, sr=44100, new_sr=16000, # new sample rate for input sample_len_s=2 # new length of input in seconds ) data.setup() . 100%|██████████| 320/320 [00:00&lt;00:00, 6023.95it/s] 100%|██████████| 80/80 [00:00&lt;00:00, 6399.73it/s] 0it [00:00, ?it/s] . Now let&#39;s view our training dataset and check that our Folds are correct (i.e. that we&#39;re not including our training/validation folds): . data.data_train.df . filename fold target category esc10 src_file take . 401 2-100786-A-1.wav | 2 | 1 | rooster | True | 100786 | A | . 402 2-101676-A-10.wav | 2 | 10 | rain | True | 101676 | A | . 418 2-102852-A-11.wav | 2 | 11 | sea_waves | True | 102852 | A | . 445 2-107351-A-20.wav | 2 | 20 | crying_baby | True | 107351 | A | . 446 2-107351-B-20.wav | 2 | 20 | crying_baby | True | 107351 | B | . ... ... | ... | ... | ... | ... | ... | ... | . 1876 5-233160-A-1.wav | 5 | 1 | rooster | True | 233160 | A | . 1888 5-234879-A-1.wav | 5 | 1 | rooster | True | 234879 | A | . 1889 5-234879-B-1.wav | 5 | 1 | rooster | True | 234879 | B | . 1894 5-235671-A-38.wav | 5 | 38 | clock_tick | True | 235671 | A | . 1999 5-9032-A-0.wav | 5 | 0 | dog | True | 9032 | A | . 320 rows × 7 columns . data.data_train.df.fold.unique() . array([2, 3, 4, 5]) . Now let´s see our validation and test set dataframes: . data.data_val.df . filename fold target category esc10 src_file take . 0 1-100032-A-0.wav | 1 | 0 | dog | True | 100032 | A | . 14 1-110389-A-0.wav | 1 | 0 | dog | True | 110389 | A | . 24 1-116765-A-41.wav | 1 | 41 | chainsaw | True | 116765 | A | . 54 1-17150-A-12.wav | 1 | 12 | crackling_fire | True | 17150 | A | . 55 1-172649-A-40.wav | 1 | 40 | helicopter | True | 172649 | A | . ... ... | ... | ... | ... | ... | ... | ... | . 366 1-81883-A-21.wav | 1 | 21 | sneezing | True | 81883 | A | . 375 1-85362-A-0.wav | 1 | 0 | dog | True | 85362 | A | . 383 1-91359-A-11.wav | 1 | 11 | sea_waves | True | 91359 | A | . 384 1-91359-B-11.wav | 1 | 11 | sea_waves | True | 91359 | B | . 392 1-97392-A-0.wav | 1 | 0 | dog | True | 97392 | A | . 80 rows × 7 columns . data.data_val.df.fold.unique() . array([1]) . data.data_test.df . filename fold target category esc10 src_file take . data.data_test.df.fold.unique() . array([], dtype=int64) . Now let&#39;s view some of the items in the datasets: . data.data_train[0] . (tensor([[[-4.1812e-05, -9.4571e-05, -2.7154e-04, ..., 9.3362e-02, 8.9986e-02, 1.2432e-02]], [[-3.4689e-01, -1.7424e-01, 4.9812e-02, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]], [[ 5.9433e-02, 1.6133e-01, 2.5748e-01, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]], [[-9.3471e-03, 2.7521e-03, 2.2814e-02, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]], [[ 0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]]]), 7) . data.data_train[0][0].shape . torch.Size([5, 1, 32000]) . So we can see that for the 32kHz dimension input network we&#39;ll have 5 consecutive overlapping frames per example. We can also see the data is normalised between -1 to +1. . data.data_train[0][0][2].min() . tensor(-0.5169) . len(data.data_val) . 80 . import IPython.display as ipd # grab dataset item sample, label = data.data_train[1][0][0], data.data_train[1][1] # display info print(f&quot;Shape of waveform: {sample.size()}&quot;) print(f&quot;class: {data.data_train.i2c[label]}&quot;) plt.figure() plt.plot(waveform.t().numpy()) ipd.Audio(sample, rate=32000) # load tensor from dataset . Shape of waveform: torch.Size([1, 32000]) class: rain . Your browser does not support the audio element. data.data_train.i2c . {0: &#39;chainsaw&#39;, 1: &#39;clock_tick&#39;, 2: &#39;crackling_fire&#39;, 3: &#39;crying_baby&#39;, 4: &#39;dog&#39;, 5: &#39;helicopter&#39;, 6: &#39;rain&#39;, 7: &#39;rooster&#39;, 8: &#39;sea_waves&#39;, 9: &#39;sneezing&#39;} . num_classes = len(data.data_train.i2c) num_classes . 10 . len(data.train_dataloader()) . 4 . Check our dataloaders by iterating to first batch - we can see the stacked frames from the full samples . x = next(iter(data.train_dataloader())) x . [tensor([[[-1.8144e-03, -1.1270e-02, 1.6327e-04, ..., -8.4655e-03, -8.6356e-03, 6.4397e-05]], [[ 1.4381e-02, -5.6551e-04, -1.6240e-03, ..., 2.1071e-04, 6.2214e-03, -8.2650e-03]], [[-2.4150e-02, -7.4060e-02, 7.0644e-02, ..., 5.4250e-03, 1.3108e-02, -5.5006e-03]], ..., [[-9.4573e-02, -9.5227e-02, -9.5475e-02, ..., 1.6938e-01, 1.6831e-01, 1.6930e-01]], [[-3.2459e-02, -3.2887e-02, -3.3784e-02, ..., -5.9647e-02, -6.9917e-02, -6.7229e-02]], [[-9.1665e-04, -7.8192e-04, 1.4184e-04, ..., -2.3076e-02, -2.4520e-02, -2.6394e-02]]]), tensor([6, 0, 1, 7, 1, 8, 6, 5, 5, 4, 6, 1, 9, 4, 7, 4, 6, 0, 5, 4, 6, 6, 7, 6, 9, 0, 1, 5, 8, 5, 1, 3, 5, 6, 2, 0, 0, 3, 2, 7, 7, 2, 9, 1, 3, 6, 7, 2, 1, 9, 5, 7, 5, 4, 1, 1, 8, 9, 7, 5, 9, 7, 4, 2, 7, 0, 4, 7, 2, 4, 1, 0, 6, 8, 3, 4, 0, 7, 9, 4, 3, 5, 3, 2, 4, 2, 5, 3, 6, 1, 1, 3, 4, 2, 1, 3, 9, 4, 2])] . We can see that we have 495 frames of audio relating to 99 different labeled examples per batch (i.e. 5 frames per example for 32000 input network): . len(x[0]), len(x[1]) . (495, 99) . Lightning Module . We&#39;ll create a Lightning Module for each network architecture, starting with the 16000 dimension input. . We can see for each network that after the training and validation step predictions on each individual sub-frames, we group the predictions for the 1s sub-frames into their overall examples and average over the predictions of all the sub-frames to get the final prediction for that example. This overall prediction per example is then fed into the cross-entropy loss calculation. . In addition, a one-cycle learning rate policy has been defined, but with a flat initial stage and cosine annealing, which was found to work best in practice. We also log our training and validation parameters to wandb for visualisation. . class audioNet16k(pl.LightningModule): # Define Model Architecture def __init__(self, num_classes=0, epochs=5, lr=1e-3, metric=None,steps_per_epoch=None): super().__init__() self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=64, stride=2) # in_channels, num_filters (out_channels), kernel_size, stride self.bn1 = nn.BatchNorm1d(16) self.pool1 = nn.MaxPool1d(kernel_size=8, stride=8) self.conv2 = nn.Conv1d(16, 32, kernel_size=32, stride=2) self.bn2 = nn.BatchNorm1d(32) self.pool2 = nn.MaxPool1d(kernel_size=8, stride=8) self.conv3 = nn.Conv1d(32, 64, kernel_size=16, stride=2) self.bn3 = nn.BatchNorm1d(64) self.conv4 = nn.Conv1d(64, 128, kernel_size=8, stride=2) self.bn4 = nn.BatchNorm1d(128) self.fc1 = nn.Linear(1024, 128) # flattened last BN layer 128*8 self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, num_classes) self.dropout = nn.Dropout(p=0.25) self.num_classes = num_classes self.epochs = epochs self.lr=lr self.metric=metric self.steps_per_epoch = steps_per_epoch self.predictions = [] self.test_set_y = [] self.val_acc = 0 def forward(self, x, *args, **kwargs): x = self.conv1(x) x = self.bn1(F.relu(x)) # batch norm after activation as per paper x = self.pool1(x) x = self.conv2(x) x = self.bn2(F.relu(x)) x = self.pool2(x) x = self.conv3(x) x = self.bn3(F.relu(x)) x = self.conv4(x) x = self.bn4(F.relu(x)) x = torch.flatten(x, start_dim=1, end_dim=-1) # flatten last conv layer before fc x = self.dropout(F.relu(self.fc1(x))) x = self.dropout(F.relu(self.fc2(x))) x = self.fc3(x) x = torch.log_softmax(x, dim=1) return x def cross_entropy_loss(self, logits, labels): return F.nll_loss(logits, labels) def count_parameters(self): return sum(p.numel() for p in self.parameters()) # Define configure_optimizers and lr_scheduling - onecycle learning rate def configure_optimizers(self): optimizer = optim.Adam(self.parameters(), lr=self.lr) lr_scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=self.lr, # single max_lr based on learning rate finder total_steps = self.epochs*self.steps_per_epoch, div_factor = 1 # for flat initial stage (instead of warmup) ) scheduler = {&quot;scheduler&quot;: lr_scheduler, &quot;interval&quot; : &quot;step&quot;} # to step at each batch rather than each epoch for 1cycle learning rates return [optimizer], [scheduler] def training_step(self, batch, batch_idx): x, y = batch y_hat = self.forward(x) y_hats = torch.split(y_hat, 9) # split every 9 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample loss = self.cross_entropy_loss(y_hat, y) self.logger.experiment.log({&#39;train_loss&#39;: loss}) return {&#39;loss&#39;: loss} def validation_step(self, batch, batch_idx): x, y = batch y_hat = self.forward(x) y_hats = torch.split(y_hat, 9) # split every 9 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample loss = self.cross_entropy_loss(y_hat, y) acc = self.metric(y_hat, y) self.logger.experiment.log({&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc}) return {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean() avg_acc = torch.stack([x[&#39;val_acc&#39;] for x in outputs]).mean() self.logger.experiment.log({&#39;avg_val_loss&#39;: avg_loss, &#39;avg_val_acc&#39;: avg_acc}) self.val_acc=avg_acc return {&#39;avg_val_loss&#39;: avg_loss, &#39;avg_val_acc&#39;: avg_acc} # test-loop, not used def test_step(self, batch, batch_idx): x, y = batch y_hat = self(x) y_hats = torch.split(y_hat, 9) # split every 9 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample result = F.softmax(y_hat, dim = 1) self.predictions.append(result) self.test_set_y.append(y) return result . from asteroid.filterbanks.enc_dec import Filterbank, Encoder, Decoder from asteroid.filterbanks.multiphase_gammatone_fb import MultiphaseGammatoneFB class audioNet16kgamma(pl.LightningModule): def __init__(self, num_classes=0, epochs=5, lr=1e-3, loss_func=None, metric=None, steps_per_epoch=None): super().__init__() self.enc = Encoder(MultiphaseGammatoneFB(n_filters=64, kernel_size=512, stride=1, sample_rate=16000)) self.bn1 = nn.BatchNorm1d(64) self.pool1 = nn.MaxPool1d(kernel_size=8, stride=8) self.conv2 = nn.Conv1d(64, 32, kernel_size=32, stride=2) self.bn2 = nn.BatchNorm1d(32) self.pool2 = nn.MaxPool1d(kernel_size=8, stride=8) self.conv3 = nn.Conv1d(32, 64, kernel_size=16, stride=2) self.bn3 = nn.BatchNorm1d(64) self.conv4 = nn.Conv1d(64, 128, kernel_size=8, stride=2) self.bn4 = nn.BatchNorm1d(128) self.fc1 = nn.Linear(2944, 128) # flattened last BN layer 128*8 self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, num_classes) self.dropout = nn.Dropout(p=0.25) self.num_classes = num_classes self.epochs = epochs self.lr=lr self.metric=metric self.steps_per_epoch = steps_per_epoch self.predictions = [] self.test_set_y = [] self.val_acc = 0 def forward(self, x, *args, **kwargs): x = self.enc(x) x = self.bn1(F.relu(x)) # batch norm after activation as per paper x = self.pool1(x) x = self.conv2(x) x = self.bn2(F.relu(x)) x = self.pool2(x) x = self.conv3(x) x = self.bn3(F.relu(x)) x = self.conv4(x) x = self.bn4(F.relu(x)) x = torch.flatten(x, start_dim=1, end_dim=-1) # flatten last pooling layer before fc x = self.dropout(self.fc1(x)) x = self.dropout(self.fc2(x)) x = self.fc3(x) x = torch.log_softmax(x, dim=1) return x def cross_entropy_loss(self, logits, labels): return F.nll_loss(logits, labels) def count_parameters(self): return sum(p.numel() for p in self.parameters()) # Define configure_optimizers and lr_scheduling - onecycle learning rate def configure_optimizers(self): optimizer = optim.Adam(self.parameters(), lr=self.lr) lr_scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=self.lr, # single max_lr based on learning rate finder total_steps = self.epochs*self.steps_per_epoch, div_factor = 1 # for flat initial stage (instead of warmup) ) scheduler = {&quot;scheduler&quot;: lr_scheduler, &quot;interval&quot; : &quot;step&quot;} # to step at each batch rather than each epoch for 1cycle learning rates return [optimizer], [scheduler] def training_step(self, batch, batch_idx): x, y = batch y_hat = self.forward(x) y_hats = torch.split(y_hat, 9) # split every 9 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample loss = self.cross_entropy_loss(y_hat, y) self.logger.experiment.log({&#39;train_loss&#39;: loss}) return {&#39;loss&#39;: loss} def validation_step(self, batch, batch_idx): x, y = batch y_hat = self.forward(x) y_hats = torch.split(y_hat, 9) # split every 9 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample loss = self.cross_entropy_loss(y_hat, y) acc = self.metric(y_hat, y) self.logger.experiment.log({&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc}) return {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean() avg_acc = torch.stack([x[&#39;val_acc&#39;] for x in outputs]).mean() self.logger.experiment.log({&#39;avg_val_loss&#39;: avg_loss, &#39;avg_val_acc&#39;: avg_acc}) self.val_acc=avg_acc return {&#39;avg_val_loss&#39;: avg_loss, &#39;avg_val_acc&#39;: avg_acc} def test_step(self, batch, batch_idx): x, y = batch y_hat = self(x) y_hats = torch.split(y_hat, 9) # split every 9 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample result = F.softmax(y_hat, dim = 1) self.predictions.append(result) self.test_set_y.append(y) return result . class audioNet32k(pl.LightningModule): def __init__(self, num_classes=0, epochs=5, lr=1e-3, loss_func=None, metric=None, steps_per_epoch=None): super().__init__() self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=64, stride=2) # in_channels, num_filters (out_channels), kernel_size, stride self.bn1 = nn.BatchNorm1d(16) self.pool1 = nn.MaxPool1d(kernel_size=8, stride=8) self.conv2 = nn.Conv1d(16, 32, kernel_size=32, stride=2) self.bn2 = nn.BatchNorm1d(32) self.pool2 = nn.MaxPool1d(kernel_size=8, stride=8) self.conv3 = nn.Conv1d(32, 64, kernel_size=16, stride=2) self.bn3 = nn.BatchNorm1d(64) self.conv4 = nn.Conv1d(64, 128, kernel_size=8, stride=2) self.bn4 = nn.BatchNorm1d(128) self.conv5 = nn.Conv1d(128, 256, kernel_size=4, stride=2) self.bn5 = nn.BatchNorm1d(256) self.pool3 = nn.MaxPool1d(kernel_size=4, stride=4) self.fc1 = nn.Linear(512, 128) # flattened last pool layer self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, num_classes) self.dropout = nn.Dropout(p=0.25) self.num_classes = num_classes self.epochs = epochs self.lr=lr self.metric=metric self.steps_per_epoch = steps_per_epoch self.predictions = [] self.test_set_y = [] self.val_acc = 0 def forward(self, x, *args, **kwargs): x = self.conv1(x) x = self.bn1(F.relu(x)) # batch norm after activation as per paper x = self.pool1(x) x = self.conv2(x) x = self.bn2(F.relu(x)) x = self.pool2(x) x = self.conv3(x) x = self.bn3(F.relu(x)) x = self.conv4(x) x = self.bn4(F.relu(x)) x = self.conv5(x) x = self.bn5(F.relu(x)) x = self.pool3(x) x = torch.flatten(x, start_dim=1, end_dim=-1) # flatten last pooling layer before fc x = self.dropout(F.relu(self.fc1(x))) x = self.dropout(F.relu(self.fc2(x))) x = self.fc3(x) x = torch.log_softmax(x, dim=1) return x def cross_entropy_loss(self, logits, labels): return F.nll_loss(logits, labels) def count_parameters(self): return sum(p.numel() for p in self.parameters()) # Define configure_optimizers and lr_scheduling - onecycle learning rate def configure_optimizers(self): optimizer = optim.Adam(self.parameters(), lr=self.lr) lr_scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=self.lr, # single max_lr based on learning rate finder total_steps = self.epochs*self.steps_per_epoch, div_factor = 1 # for flat initial stage (instead of warmup) ) scheduler = {&quot;scheduler&quot;: lr_scheduler, &quot;interval&quot; : &quot;step&quot;} # to step at each batch rather than each epoch for 1cycle learning rates return [optimizer], [scheduler] def training_step(self, batch, batch_idx): x, y = batch y_hat = self.forward(x) y_hats = torch.split(y_hat, 5) # split every 5 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample loss = self.cross_entropy_loss(y_hat, y) self.logger.experiment.log({&#39;train_loss&#39;: loss}) return {&#39;loss&#39;: loss} def validation_step(self, batch, batch_idx): x, y = batch y_hat = self.forward(x) y_hats = torch.split(y_hat, 5) # split every 5 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample loss = self.cross_entropy_loss(y_hat, y) acc = self.metric(y_hat, y) self.logger.experiment.log({&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc}) return {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean() avg_acc = torch.stack([x[&#39;val_acc&#39;] for x in outputs]).mean() self.logger.experiment.log({&#39;avg_val_loss&#39;: avg_loss, &#39;avg_val_acc&#39;: avg_acc}) self.val_acc=avg_acc return {&#39;avg_val_loss&#39;: avg_loss, &#39;avg_val_acc&#39;: avg_acc} def test_step(self, batch, batch_idx): x, y = batch y_hat = self(x) y_hats = torch.split(y_hat, 5) # split every 9 frames (i.e. each sample) y_hat = torch.stack([torch.mean(yhat, 0) for yhat in y_hats]) # take all yhat predictions and average over all 9 subframes per sample result = F.softmax(y_hat, dim = 1) self.predictions.append(result) self.test_set_y.append(y) return result . Check models . Let&#39;s check the number of parameters against the original paper: . from pytorch_lightning import Trainer, seed_everything from pytorch_lightning.callbacks import LearningRateLogger # sets seeds for numpy, torch, python.random and PYTHONHASHSEED. seed_everything(42) net16 = audioNet16k(num_classes=num_classes) net16G = audioNet16kgamma(num_classes=num_classes) net32 = audioNet32k(num_classes=num_classes) . net16.count_parameters() . 256538 . net16G.count_parameters() . 550506 . net32.count_parameters() . 322842 . Bang on, and very lightweight! . 5-Fold Cross Validation . Now we&#39;ll train for 100 epochs and use 5-Fold cross validation accuracy across all folds of the data. . We&#39;ll do this for all networks. . Note: the learning rates were found via use of the PyTorch Lightning learning rate finder. The optimum batch sizes were found via trial and error, and the restrictions of the GPU for the Gammatone Filter bank initialised network. . from pytorch_lightning import Trainer, seed_everything from pytorch_lightning.callbacks import LearningRateLogger lr_monitor = LearningRateLogger(logging_interval=&#39;step&#39;) # sets seeds for numpy, torch, python.random and PYTHONHASHSEED. seed_everything(42) accuracies = [] models= [&quot;net16&quot;,&quot;net16G&quot;,&quot;net32&quot;] epochs = 100 for model in models: for i in range(1,6): print(f&quot;Valid fold: {i}&quot;) wandb_logger = WandbLogger(name=f&#39;{model}-Fold-{i}-Cross-Validation-100-Epochs&#39;,project=&#39;pl-1D-CNN-ESC-10&#39;) # instantiate new model on each fold if model == &quot;net16&quot;: data = DataModule(data_dir=path, df=df, valid_fold=i, test_fold=0, # set to 0 for no test set esc10=True, file_col=&#39;filename&#39;, label_col=&#39;category&#39;, batch_size=64, num_workers=8, sr=44100, new_sr=16000, sample_len_s=1 ) data.setup() net = audioNet16k(num_classes=num_classes, epochs=epochs, lr=1e-3, # chosen via lightning learning rate finder metric=Accuracy(), steps_per_epoch=len(data.train_dataloader())) elif model == &quot;net16G&quot;: data = DataModule(data_dir=path, df=df, valid_fold=i, test_fold=0, # set to 0 for no test set esc10=True, file_col=&#39;filename&#39;, label_col=&#39;category&#39;, batch_size=64, num_workers=8, sr=44100, new_sr=16000, sample_len_s=1 ) data.setup() net = audioNet16kgamma(num_classes=num_classes, epochs=epochs, lr=7e-4, # chosen via lightning learning rate finder metric=Accuracy(), steps_per_epoch=len(data.train_dataloader())) elif model == &quot;net32&quot;: data = DataModule(data_dir=path, df=df, valid_fold=i, test_fold=0, # set to 0 for no test set esc10=True, file_col=&#39;filename&#39;, label_col=&#39;category&#39;, batch_size=64, num_workers=8, sr=44100, new_sr=16000, sample_len_s=2 ) data.setup() net = audioNet32k(num_classes=num_classes, epochs=epochs, lr=1e-3, metric=Accuracy(), steps_per_epoch=len(data.train_dataloader())) # Start training loop trainer = Trainer(gpus=1, logger=wandb_logger, deterministic=True, max_epochs=net.epochs, auto_lr_find=False, callbacks=[lr_monitor]) trainer.fit(net, data) accuracies.append(net.val_acc) print(f&quot;5-Fold accuracy: {sum([acc.item() for acc in accuracies])/5}&quot;) . Results . Following the Training, let&#39;s see some of our wandb-logged graphs. . First of all, let&#39;s check the learning rates we supplied to the optimizers - flat initial stage with cosine-annealing. . . Now let&#39;s see our training curves . . Unfortunately, the 16K Gammatone Filterbank Initialised network training failed and, as such, we only have results for the first 3 folds. However, we can clearly see that it is typically inferior to the standard convolutional networks and therefore requires further investigation to improve the performance. . ESC10 Results - All Folds . Our final accuracies are as follows: . Model Fold 1 Avg Val Acc Fold 2 Avg Val Acc Fold 3 Avg Val Acc Fold 4 Avg Val Acc Fold 5 Avg Val Acc 5 Fold Avg Acc . 16K | 0.836 | 0.797 | 0.828 | 0.867 | 0.813 | 0.828 | . 32K | 0.828 | 0.813 | 0.836 | 0.789 | 0.836 | 0.82 | . 16K Gamma FB | 0.789 | 0.773 | 0.813 | NA | NA | NA | . Clearly, the 16K model wins out. . The performance demonstrated here, for such a small network, a relatively small dataset and a relatively small amount of training is impressive at 83% accuracy. While vastly outperformed by a 2D ResNet18 with mel-spectrograms, a network with a 36x size reduction could potentially be of benefit in on-device applications where resources are limited, e.g. sound level meters to automatically classify the ambient noise into basic categories, for flagging specific instances of noises of interest such as sirens or plant noise that are either the subject of interest, or need to be removed from the measurement. . Further work will be to investigate the potential for ResNet-type versions of the models to see if the performance is improved with residual blocks, but still keeping the parameter count small. .",
            "url": "https://mikful.github.io/blog/pytorch/pytorch-lightning/audio/cnn/1d/2020/11/02/1Dcnn-end-to-end-16-32k-gamma-pl.html",
            "relUrl": "/pytorch/pytorch-lightning/audio/cnn/1d/2020/11/02/1Dcnn-end-to-end-16-32k-gamma-pl.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "fastai to PyTorch to Lightning - Part 2",
            "content": "Having been a user of fastai for a while, I figured it was time to try to learn PyTorch properly. As a learning exercise, I decided to try to recreate a fastai image classifier system using transfer learning within Pytorch and having also recently read about PyTorch Lightning, then take the PyTorch model and recreate it within Lightning. This blog series will therefore be in two parts, of which this is Part 2: . fastai to PyTorch | PyTorch to PyTorch Lightning | In part 1, we saw how easy it was to achieve 98.5% accuracy on the Imagenette dataset in less than 10 minutes and 10 epochs of training using fastai, and 97.9% accuracy with a bit more work within PyTorch by trying to recreate the fastai transforms, transfer learning model and staged-training (frozen then fine-tuned). The PyTorch model also took slightly longer to train at 11.5 minutes for the 10 epochs. . PyTorch Lightning . Now that we&#39;ve trained our model with transfer learning using straight PyTorch, we&#39;ll try to repeat this using PyTorch Lighting. . A PyTorch Lightning model comprises: . Model | Optimizer | Data | Training Loop &quot;the magic&quot; (according to the docs!) | Validation Loop &quot;the validation magic&quot; (also according to the docs!) | However, in Lightning we define these all within the Lightning Modules, which are PyTorch nn.Modules but with many convenience functions and improvements. . import torch import pytorch_lightning as pl from pytorch_lightning.metrics import Accuracy from torch import nn, optim from torchvision import datasets, transforms from torch.utils.data import random_split, DataLoader import os from pathlib import Path . Define DataModule . The Lightning DataModule is optional but makes it easy for us to share our dataloading techniques, such as: . Our transforms | Our splits | Normalization | How the data was prepared | Batch sizes | DataLoaders | . https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html . Essential we need to create a class and give it attributes and methods for the setup we&#39;ve already made in PyTorch. (In this case, I had to shuffle the validation dataset also, as otherwise all of the targets would have been 0&#39;s for the first batches, leading to a 0 target error. . class DataModule(pl.LightningDataModule): def __init__(self, data_dir, batch_size, num_workers): super().__init__() self.data_dir = data_dir self.batch_size = batch_size self.num_workers = num_workers self.transforms_train = transforms.Compose([ transforms.RandomHorizontalFlip(), transforms.RandomPerspective(0.2), transforms.RandomResizedCrop(224, scale=(0.08, 0.75)), transforms.ColorJitter(brightness=0.2, contrast=0.2), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet stats ]) self.transforms_val = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) def setup(self, stage = None): if stage == &#39;fit&#39; or stage is None: self.data_train = datasets.ImageFolder(self.data_dir/&quot;train&quot;, transform = self.transforms_train) self.data_val = datasets.ImageFolder(self.data_dir/&quot;val&quot;, transform = self.transforms_val) def train_dataloader(self): return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=True, # files stored by target - to avoid all targets being 0 on first batches num_workers=self.num_workers, pin_memory=True) def val_dataloader(self): return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=True, # files stored by target - to avoid all targets being 0 on first batches num_workers=self.num_workers, pin_memory=True) . We can now instantiate our DataModule and view the train and validation datasets as before: . data = DataModule(data_dir, batch_size=64, num_workers=2) data.setup() . data . &lt;__main__.DataModule at 0x7fc172f46c50&gt; . data.data_train . Dataset ImageFolder Number of datapoints: 9469 Root location: /root/.fastai/data/imagenette2-160/train StandardTransform Transform: Compose( RandomHorizontalFlip(p=0.5) RandomPerspective(p=0.5) RandomResizedCrop(size=(224, 224), scale=(0.08, 0.75), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR) ColorJitter(brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=None, hue=None) ToTensor() Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ) . data.data_val . Dataset ImageFolder Number of datapoints: 3925 Root location: /root/.fastai/data/imagenette2-160/val StandardTransform Transform: Compose( Resize(size=256, interpolation=PIL.Image.BILINEAR) CenterCrop(size=(224, 224)) ToTensor() Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ) . We can also see information about the items as before: . data.data_train.classes . [&#39;n01440764&#39;, &#39;n02102040&#39;, &#39;n02979186&#39;, &#39;n03000684&#39;, &#39;n03028079&#39;, &#39;n03394916&#39;, &#39;n03417042&#39;, &#39;n03425413&#39;, &#39;n03445777&#39;, &#39;n03888257&#39;] . num_classes = len(data.data_train.classes) num_classes . 10 . data.batch_size . 64 . data.data_train[1] . (tensor([[[-0.6281, -0.6281, -0.6452, ..., -1.5870, -1.8268, -1.8268], [-0.6281, -0.6281, -0.6452, ..., -1.5870, -1.8268, -1.8268], [-0.6281, -0.6281, -0.6452, ..., -1.6213, -1.8439, -1.8439], ..., [-1.0733, -1.0733, -1.1075, ..., 0.9988, 1.0331, 1.0331], [-1.0048, -1.0048, -1.0390, ..., 1.0844, 1.1015, 1.1015], [-1.0048, -1.0048, -1.0390, ..., 1.0844, 1.1015, 1.1015]], [[-0.4601, -0.4601, -0.4601, ..., -1.4580, -1.6856, -1.6856], [-0.4601, -0.4601, -0.4601, ..., -1.4580, -1.6856, -1.6856], [-0.4601, -0.4601, -0.4601, ..., -1.4930, -1.7206, -1.7206], ..., [-1.1604, -1.1604, -1.1779, ..., 0.6254, 0.6429, 0.6429], [-1.0903, -1.0903, -1.1253, ..., 0.6954, 0.7129, 0.7129], [-1.0903, -1.0903, -1.1253, ..., 0.6954, 0.7129, 0.7129]], [[-0.6541, -0.6541, -0.6541, ..., -1.3339, -1.5779, -1.5779], [-0.6541, -0.6541, -0.6541, ..., -1.3339, -1.5779, -1.5779], [-0.6541, -0.6541, -0.6541, ..., -1.3687, -1.5779, -1.5779], ..., [-1.2641, -1.2641, -1.2816, ..., 0.2522, 0.2522, 0.2522], [-1.2119, -1.2119, -1.2467, ..., 0.3219, 0.3219, 0.3219], [-1.2119, -1.2119, -1.2467, ..., 0.3219, 0.3219, 0.3219]]]), 0) . steps_per_epoch = len(data.train_dataloader()) steps_per_epoch . 148 . Check a a batch of our target values: . inputs, targets = next(iter(data.train_dataloader())) . targets . tensor([0, 0, 1, 7, 6, 0, 0, 5, 3, 9, 4, 3, 3, 7, 1, 3, 1, 2, 2, 1, 0, 5, 8, 5, 5, 4, 3, 8, 9, 9, 3, 2, 1, 8, 6, 2, 9, 6, 7, 7, 4, 5, 8, 2, 7, 7, 9, 2, 2, 1, 0, 8, 4, 2, 1, 9, 2, 5, 6, 8, 9, 2, 0, 9]) . Visualise Batch of Images . As before, we can visualise some images: . def imshow(inp, title=None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) # plt.figure(figsize = (20,6)) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(data.train_dataloader())) # Choose number items to display n_items = 4 # Make a grid from batch out = torchvision.utils.make_grid(inputs[:n_items]) imshow(out, title=[data.data_train.classes[x] for x in classes[:n_items]]) . Lightning Module . The main Lightning Module contains our model definition and our training loop. It also takes care of the following automatically, which greatly simplifies the training process and order of operations - saving us potential errors in the gradient accumulation or step (or lack thereof!). In addition, we can define our losses and metrics to be tracked, for printing to Tensorboard later. . auto sets model.train() for Dropout or BatchNorm | auto sets model.eval() for validation | auto sets torch.no_grad() for logits in validation set to disable gradient accumulation | automatically put tensors on correct devices | automatically does backward() step for gradient accumulation | track all losses and accuracies if set | detach automatically when returning Loss | automatic learning rate finding | . Note that we do not need to change our transforms or dataloaders, as PyTorch Lighting is pure PyTorch under the hood. However, we have defined our DataModule above for reproducibility. . Note also how different optimizers and lr_schedulers have been defined based on the frozen state of the model i.e. such that we can use discriminative learning rates for different layers of the model as per fastai. . So let&#39;s define our Lightning Module. We&#39;ll first create our AdaptiveConcatPool2D layer as before. . class AdaptiveConcatPool2d(nn.Module): &quot;Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`&quot; def __init__(self, size=None): super(AdaptiveConcatPool2d, self).__init__() self.size = size or 1 self.ap = nn.AdaptiveAvgPool2d(self.size) self.mp = nn.AdaptiveMaxPool2d(self.size) def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1) . Now, we put everything to do with our architecture, losses, metrics, optimizers learning rate scheduling and any other arbitrary functions, such as freeze and unfreeze functions for a staged training method using fine-tuning as we did with fastai and the PyTorch model. . # pl.LightningModule = same as nn.Module but with added methods class transferLearner(pl.LightningModule): # Define Model Architecture as before def __init__(self, model=None, num_classes=0, epochs=5, lr=3e-3, batch_size=64, frozen_body=False, loss_func=None, metric=None): super().__init__() # fastai equivalent pretrained body self.model_body = model # fastai equivalent head self.model_head = nn.Sequential(AdaptiveConcatPool2d(), nn.Flatten(), nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Dropout(p=0.25, inplace=False), nn.Linear(in_features=1024, out_features=512, bias=False), nn.ReLU(inplace=True), nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Dropout(p=0.5, inplace=False), nn.Linear(in_features=512, out_features=num_classes, bias=False), ) self.num_classes = num_classes self.epochs = epochs self.lr=lr self.batch_size=batch_size self.frozen_body=frozen_body self.loss_func=loss_func self.metric=metric # define our forward loop as before def forward(self, x, *args, **kwargs): out = self.model_body(x) out = self.model_head(out) return out # Define our freeze and unfreeze functions as before def freeze(self): for name, param in self.model_body.named_parameters(): if(&quot;bn&quot; not in name): param.requires_grad = False # print(f&#39;{name} requires_grad state: {param.requires_grad}&#39;) self.frozen_body = True print(f&#39;Model frozen: {self.frozen_body}&#39;) print(f&#39;{sum(p.numel() for p in self.parameters() if p.requires_grad)} trainable parameters.&#39;) def unfreeze(self): for param in self.parameters(): param.requires_grad = True self.frozen_body = False print(f&#39;Model frozen: {self.frozen_body}&#39;) print(f&#39;{sum(p.numel() for p in self.parameters() if p.requires_grad)} trainable parameters.&#39;) # Define configure_optimizers method by depending on model&#39;s frozen state (i.e. head training / fine-tune) # fine-tune: pass network&#39;s body and head parameters separately for both optimizer and lr scheduler # and discriminative layer lr_scheduling def configure_optimizers(self): if self.frozen_body: optimizer = optim.Adam(self.parameters(), lr=self.lr) lr_scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=self.lr, # single max_lr based on learning rate finder total_steps= self.epochs * steps_per_epoch, epochs=self.epochs ) scheduler = {&quot;scheduler&quot;: lr_scheduler, &quot;interval&quot; : &quot;step&quot; } # to step at each batch rather than each epoch for 1cycle learning rates elif self.frozen_body == False: optimizer = optim.Adam([{&quot;params&quot;: self.model_body.parameters(), &quot;lr&quot;: self.lr}, {&quot;params&quot;: self.model_head.parameters(), &quot;lr&quot;: self.lr}]) lr_scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=[self.lr/100, self.lr], # grouped max_lr&#39;s for body and head total_steps= self.epochs * steps_per_epoch, epochs=self.epochs ) scheduler = {&quot;scheduler&quot;: lr_scheduler, &quot;interval&quot;: &quot;step&quot; } # to step at each batch rather than each epoch for 1cycle learning rates return [optimizer], [scheduler] # Print new progress bar each epoch def on_epoch_start(self): print(&#39; n&#39;) # Define training_step training loop method def training_step(self, batch, batch_idx): x, y = batch y_hat = self(x) loss = self.loss_func(y_hat, y) result = pl.TrainResult(loss) result.log(&#39;train_loss:&#39;, loss, on_step=False, on_epoch=True, prog_bar=True, logger=True) return result # Create a validation_step hook by calling the training_step method # i.e. for batch return the loss and acc and cache it # returned to validation_epoch_end def validation_step(self, batch, batch_idx): x, y = batch y_hat = self(x) loss = self.loss_func(y_hat, y) acc = self.metric(y_hat, y) result = pl.EvalResult() result.valid_batch_loss = loss result.valid_batch_acc = acc result.log(&#39;valid_loss:&#39;, loss, on_epoch=True, prog_bar=True, logger=True) return result # Get our average loss and accuracy def validation_epoch_end(self, outputs): avg_loss = outputs.valid_batch_loss.mean() avg_acc = outputs.valid_batch_acc.mean() result = pl.EvalResult(checkpoint_on=avg_loss) result.log_dict({&#39;val_loss&#39;: avg_loss, &#39;val_acc&#39;: avg_acc}, on_epoch=True, prog_bar=True) return result . def create_model_body(model, cut=-2): model_body_modules = list(model.children())[:cut] return nn.Sequential(*model_body_modules) model_body = create_model_body(model=torchvision.models.resnet18(pretrained=True)) . Train our model . Again let&#39;s try 5 epochs frozen and 5 unfrozen fine-tuned. . We&#39;ll set a seed for reproducibility: . from pytorch_lightning import Trainer, seed_everything from pytorch_lightning.callbacks import LearningRateLogger # sets seeds for numpy, torch, python.random and PYTHONHASHSEED. seed_everything(42) # Create our hyperparameters and instantiate model batch_size = 64 data = DataModule(data_dir, batch_size=batch_size, num_workers=8) model = transferLearner(model=model_body, num_classes=num_classes, epochs=5, lr=3e-3, batch_size=batch_size, frozen_body=True, loss_func=nn.CrossEntropyLoss(), metric=Accuracy()) . Let&#39;s ensure all the body parameters are frozen such that it&#39;s a fixed-feature extractor tp traom tje custom head initially: . model.freeze() . Model frozen: True 540160 trainable parameters. . Now let&#39;s find our learning rate with the lr finder, just as we did with fastai. . Note: This is for the frozen pretrained network and unfrozen custom head as self.frozen_body = True . trainer = Trainer(gpus=1, deterministic=True) # Run learning rate finder on our data lr_finder = trainer.lr_find(model, data) # Results can be found in lr_finder.results # Plot with fig = lr_finder.plot(suggest=True) fig.show() # Pick point based on plot, or get suggestion suggested_lr = lr_finder.suggestion() print(suggested_lr) . GPU available: True, used: True TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params 0 | model_body | Sequential | 11 M 1 | model_head | Sequential | 532 K 2 | loss_func | CrossEntropyLoss | 0 3 | metric | Accuracy | 0 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders. warnings.warn(*args, **kwargs) . . Saving latest checkpoint.. . 0.0009120108393559097 . In this case, we already knew our learning rate would be 3e-3 as per fastai, but it&#39;s good to see the lr_finder matches. . Now let&#39;s train our custom head and monitor the learning rate, such that we can check it is cycling correctly in the tensorboard. We also turn auto_lr_find=False as this is called automatically during the initial training otherwise, whereas we want to set our learning rate manually to the same as the fastai and PyTorch models for a direct comparison. . lr_monitor = LearningRateLogger(logging_interval=&#39;step&#39;) trainer = Trainer(gpus=1, max_epochs=model.epochs, deterministic=True, callbacks=[lr_monitor], auto_lr_find=False) trainer.fit(model, data) . GPU available: True, used: True TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given warnings.warn(*args, **kwargs) | Name | Type | Params 0 | model_body | Sequential | 11 M 1 | model_head | Sequential | 532 K 2 | loss_func | CrossEntropyLoss | 0 3 | metric | Accuracy | 0 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders. warnings.warn(*args, **kwargs) . . Saving latest checkpoint.. . . 1 . Now unfreeze, find the new maximum learning rate (for discriminative layer rates in transferNet) and train further: . model.unfreeze() . Model frozen: False 11708992 trainable parameters. . Note: this time note discriminative learning rates are being using within the transferNet lr_scheduler due to model_frozen = False and model.unfreeze() . trainer = Trainer(gpus=1, deterministic=True) # Run learning rate finder on our data lr_finder = trainer.lr_find(model,data) # Results can be found in lr_finder.results # Plot with fig = lr_finder.plot(suggest=True) fig.show() # Pick point based on plot, or get suggestion suggested_lr = lr_finder.suggestion() print(suggested_lr) . GPU available: True, used: True TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params 0 | model_body | Sequential | 11 M 1 | model_head | Sequential | 532 K 2 | loss_func | CrossEntropyLoss | 0 3 | metric | Accuracy | 0 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders. warnings.warn(*args, **kwargs) . . Saving latest checkpoint.. LR finder stopped early due to diverging loss. . 5.75439937337157e-07 . In this case, I had set the discrminative learning rate parameter directly within the Lightning Module above, such that the learning rate for the pretrained body was the custom head learning rate / 100. So now we&#39;ll set the main learning rate. . model.lr = 3e-4 . lr_monitor = LearningRateLogger(logging_interval=&#39;step&#39;) trainer = Trainer(gpus=1, max_epochs=5, deterministic=True, callbacks=[lr_monitor], auto_lr_find=False) trainer.fit(model, data) . GPU available: True, used: True TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given warnings.warn(*args, **kwargs) | Name | Type | Params 0 | model_body | Sequential | 11 M 1 | model_head | Sequential | 532 K 2 | loss_func | CrossEntropyLoss | 0 3 | metric | Accuracy | 0 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders. warnings.warn(*args, **kwargs) . . Saving latest checkpoint.. . . 1 . We can see from the tensorboard output that the learning rates were cycling correctly. Initially with a single maximum learning rate for the custom head training and then within two parameter groups (pg1 / pg2) for the discriminative layer learning rates in the second fine-tuning stage. We can also see our final validation accuracy. . . Results . Using PyTorch Lightning we&#39;ve acheived a final validation accuracy of: 98.0% (in just over 10 mins) essentially equaling the PyTorch performance as it should (being the same code under the hood), although it seemed to train slightly faster. . Final Results . So let&#39;s look at the final results: . fastai - 98.5% | PyTorch Lightning - 98.0% | PyTorch - 97.9% | . So with fastai we acheived the highest validation accuracy, of a 0.6% / 0.5% improvement above the PyTorch and Lighting implementations, but in a much more straightforward manner. In addition, due to the GPU batch transforms it trained much faster. . Conclusion . Clearly, fastai has the edge in terms of speed and ease of training a state-of-the-art image classification model, which likely holds true for tasks in all of the standard pipelines it provides (Image Classification, Text, Collaborative Filtering, Tabular Data). However, for more unorthdox pipelines, or investigation into new architectures and methods of training, PyTorch Lightning clearly has the edge over both fastai and PyTorch (although the fastai version 2 API makes this much more straightforward) and I will definitely be using it for any personal research in the future. The benefits for me are as follows: . Greatly simplifies the training and validation loop over pure PyTorch | Has great features such as the learning rate finder, Tensorboard integration | Clear PyTorch syntax makes it far easier to get under-the-hood than fastaiand and more understandable in terms of the implementation details | . This has been a really useful learning exercise for me and I&#39;m sure I&#39;ve only just scratched the surface of using PyTorch and PyTorch Lightning, so hopefully will be making more posts using them soon. . I hope this was informative, if you have any queries or comments, I would love to hear them, so please do let me know. .",
            "url": "https://mikful.github.io/blog/fastai/jupyter/pytorch/pytorch-lightning/2020/09/21/fastai-pytorch-lightning-part2.html",
            "relUrl": "/fastai/jupyter/pytorch/pytorch-lightning/2020/09/21/fastai-pytorch-lightning-part2.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "fastai to PyTorch to Lightning - Part 1",
            "content": "Having been a user of fastai for a while, I figured it was time to try to learn PyTorch properly. As a learning exercise, I decided to try to recreate a fastai image classifier system using transfer learning within Pytorch and, having also recently read about PyTorch Lightning, then take the PyTorch model and recreate it within PyTorch Lightning. This blog series will therefore be in two parts, of which this is Part 1: . fastai to PyTorch | PyTorch to PyTorch Lightning | I found this exercise to be very beneficial for a number of reasons: . I came to understand training a PyTorch model in far greater detail than I had before | I learned about the trickiness of training a pure PyTorch model to high accuracy vs the by-default implemented best-practices of fastai for image classification | Having seen how pure PyTorch model is implemented, I came to appreciate the benefits of organising my code into PyTorch Lightning modules | For the comparison we will use the Imagenette dataset, a subset of 10 classes from the original ImageNet dataset devised by fastai to aid the implementation of new algorithms, which is simple enough for the exercise. . fastai . Let&#39;s first of all set a benchmark performance using fastai&#39;s high level API ImageDataLoaders and cnn_learner functions. . Download Dataset and Define Data Path . data_dir = untar_data(URLs.IMAGENETTE_160) . from fastai.vision.all import * . The main benefit of the fastai API is that best-practices are instantiated by default, allowing you to get SOTA results extremely quickly on the main task domains the API is designed for (vision, text and tabular) and with a little extra work elsewhere. . A great example of this is the image transformation pipeline, that&#39;s well described in part 5 of the fastai fastbook. Namely, this incorporates the following benefits which to lead to excellent results: . &quot;Presizing&quot; - i.e. take a larger section of the image on the item transforms (which creates uniform image sizes before they are passed to the GPU for batch transformations) | the *aug_transforms which are a mixture of random flips, warps, brightness and colour changes and random resizing and cropping that are undertaken on the GPU for speed and also in one combined augmentation step, rather than many different steps which leads to interpolation artifacts. | Normalization by the ImageNet statistics as we are using a pretrained Resnet50, with transfer learning. | On the validation set, by default a centre crop of the images will be taken. . dls = ImageDataLoaders.from_folder(data_dir, valid=&#39;val&#39;, item_tfms=RandomResizedCrop(224, min_scale=0.75), batch_tfms=[*aug_transforms(),Normalize.from_stats(*imagenet_stats)]) . dls.show_batch() . Another main benefit of fastai is that we can create a transfer learning CNN with a pretrained body (a pretrained resnet18 from the PyTorch Model zoo here), custom head and automatically defined loss function (based on the dataset) and optimizer extremely easily (CrossEntropyLossFlat and Adam, respectively, in this case). Within this custom head, fastai also implements an AdaptiveConcatPool layer, which essentially concatenates MaxPool and AveragePool layers, such that we can extract more information from the dataset. We&#39;ll see the PyTorch implementation of this later. The pretrained base model is automatically frozen other than the BatchNorm layers, such that it acts as a fixed-feature extractor for the custom head in the initial training stage before fine-tuning both the head and body&#39;s weights. . learn = cnn_learner(dls, resnet18, metrics=accuracy) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . Once more, we see a benefit of fastai - a very handy learning rate finder, which is critical to defining the correct maximum learning rate during training. . learn.lr_find() . SuggestedLRs(lr_min=0.014454397559165954, lr_steep=0.0014454397605732083) . Another of fastai&#39;s useful functions is the one-cycle policy training loop, which automatically implements Leslie Smith&#39;s cosine annealing one-cycle learning rate schedule over the full training duration, while inversely varying the Adam optimizer momentum in another schedule. . learn.fit_one_cycle(5, lr=3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.464302 | 0.083805 | 0.974522 | 00:47 | . 1 | 0.158117 | 0.061372 | 0.980637 | 00:47 | . 2 | 0.108517 | 0.058426 | 0.980382 | 00:47 | . 3 | 0.074885 | 0.054388 | 0.982930 | 00:47 | . 4 | 0.062770 | 0.050900 | 0.983694 | 00:47 | . We can see that we&#39;ve acheived a very high accuracy very quickly. So let&#39;s now unfreeze the base model, find a new learning rate and fine-tune the model. . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=2.7542287170945203e-07, lr_steep=6.309573450380412e-07) . We see once more a nice easy implementation here - discriminative learning rates for the pretrained resnet and the custom head of the model, by the slice method, such that we don&#39;t damage the pretrained weights by using a too-high learning rate. . learn.fit_one_cycle(5, lr_max=slice(3e-6, 3e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.065692 | 0.051162 | 0.984204 | 00:57 | . 1 | 0.059067 | 0.055423 | 0.983185 | 00:57 | . 2 | 0.052255 | 0.057830 | 0.984459 | 00:57 | . 3 | 0.041012 | 0.052572 | 0.984204 | 00:57 | . 4 | 0.030924 | 0.052217 | 0.984968 | 00:57 | . So, we acheived a final accuracy of 98.5% in 10 epochs, taking less then 9 minutes training overall using fastai. . PyTorch . Now we have our fastai benchmark, we&#39;ll run through how I tried to recreate this performance (or as close as we can!) within pure PyTorch in the same amount of epochs. The following code is partially based on the following tutorials: . https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html . https://github.com/Paperspace/PyTorch-101-Tutorial-Series . Define Transforms . Trying to recreate the transforms within the fastai vision learner is extremely hard to do, as the batch_tfms take place on the GPU within fastai, but take place on the CPU in Pytorch and in sequential steps. . Note: Having tried many different options for the image augmentations, I settled on those detailed in the code below. It was interesting to note the huge slowdown in Google Colab created by using the transforms.Resize function to try to recreate the &quot;presizing&quot; of the fastai transforms mentioned above. The following code took over 14 minutes per 5 epochs and produced no benefit over the final transforms with it: . # 0.980 accuracy and slow training (14mins / 5 epochs) data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.Resize(460), transforms.RandomPerspective(0.2), transforms.RandomResizedCrop(224), transforms.ColorJitter(brightness=0.2, contrast=0.2), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet stats ]), &#39;val&#39;: transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } . data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.RandomHorizontalFlip(), transforms.RandomPerspective(0.2), transforms.RandomResizedCrop(224, scale=(0.08, 0.75)), transforms.ColorJitter(brightness=0.2, contrast=0.2), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet stats ]), &#39;val&#39;: transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } . Define Datasets and DataLoaders . Custom dataset method . We could define our own custom dataset as shown for comparison, however, for convenience we&#39;ll use the built in ImageFolder method within PyTorch&#39;s datasets class (further below) . class imagenetteDataset(torch.utils.data.Dataset): def __init__(self, data_dir, data_size = 0, transforms = None): files = data_dir.glob(&#39;**/*&#39;) files = [x for x in files if x.is_file()] if data_size &lt; 0 or data_size &gt; len(files): assert(&quot;Data size should be between 0 to number of files in the dataset&quot;) if data_size == 0: data_size = len(files) self.data_size = data_size self.files = random.sample(files, self.data_size) self.transforms = transforms def __len__(self): return self.data_size def __getitem__(self, idx): image_address = self.files[idx] image = Image.open(image_address) if self.transforms==None: print(&quot;No transforms defined, please define and re-run.&quot;) elif self.transforms: image = self.transforms(image) label = image_address.parent.name return image, label . dataset.ImageFolder method . https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder . This is a much more straightforward pipeline to implement, but with extra convenience attributes and methods over the above custom dataset e.g. .classes, for datasets in the following format: . root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png . image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [&#39;train&#39;, &#39;val&#39;]} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64, shuffle=True, num_workers=4) for x in [&#39;train&#39;, &#39;val&#39;]} dataset_sizes = {x: len(image_datasets[x]) for x in [&#39;train&#39;, &#39;val&#39;]} class_names = image_datasets[&#39;train&#39;].classes num_classes = len(image_datasets[&#39;train&#39;].classes) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . We can now view information about our dataset: . image_datasets[&#39;train&#39;] . Dataset ImageFolder Number of datapoints: 9469 Root location: /root/.fastai/data/imagenette2-160/train StandardTransform Transform: Compose( RandomHorizontalFlip(p=0.5) RandomPerspective(p=0.5) RandomResizedCrop(size=(224, 224), scale=(0.08, 0.75), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR) ColorJitter(brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=None, hue=None) ToTensor() Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ) . image_datasets[&#39;val&#39;] . Dataset ImageFolder Number of datapoints: 3925 Root location: /root/.fastai/data/imagenette2-160/val StandardTransform Transform: Compose( Resize(size=256, interpolation=PIL.Image.BILINEAR) CenterCrop(size=(224, 224)) ToTensor() Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ) . image_datasets[&#39;train&#39;][0] . (tensor([[[-0.0458, -0.0458, -0.0287, ..., 1.0844, 0.8447, 0.7762], [-0.0458, -0.0458, -0.0287, ..., 1.0844, 0.8618, 0.7933], [-0.0458, -0.0458, -0.0287, ..., 1.0331, 0.9303, 0.8789], ..., [-0.6623, -0.6623, -0.6965, ..., -2.1179, -2.1179, -2.1179], [-0.6965, -0.6794, -0.6281, ..., -2.1179, -2.1179, -2.1179], [-0.6965, -0.6794, -0.6281, ..., -2.1179, -2.1179, -2.1179]], [[ 0.1352, 0.1352, 0.1527, ..., 1.1331, 0.8004, 0.7129], [ 0.1352, 0.1352, 0.1527, ..., 1.1331, 0.8179, 0.7304], [ 0.1176, 0.1176, 0.1527, ..., 1.0630, 0.8880, 0.8354], ..., [-0.2850, -0.2850, -0.3025, ..., -2.0357, -2.0357, -2.0357], [-0.2850, -0.2675, -0.2150, ..., -2.0357, -2.0357, -2.0357], [-0.2850, -0.2675, -0.1975, ..., -2.0357, -2.0357, -2.0357]], [[-0.4624, -0.4624, -0.4624, ..., 0.7576, 0.4265, 0.3219], [-0.4624, -0.4624, -0.4624, ..., 0.7576, 0.4614, 0.3393], [-0.4624, -0.4624, -0.4450, ..., 0.7228, 0.5485, 0.4962], ..., [-0.5321, -0.5321, -0.5670, ..., -1.8044, -1.8044, -1.8044], [-0.6541, -0.6367, -0.6193, ..., -1.8044, -1.8044, -1.8044], [-0.6715, -0.6541, -0.6193, ..., -1.8044, -1.8044, -1.8044]]]), 0) . image_datasets[&#39;train&#39;][0][0].shape . torch.Size([3, 224, 224]) . class_names, num_classes . ([&#39;n01440764&#39;, &#39;n02102040&#39;, &#39;n02979186&#39;, &#39;n03000684&#39;, &#39;n03028079&#39;, &#39;n03394916&#39;, &#39;n03417042&#39;, &#39;n03425413&#39;, &#39;n03445777&#39;, &#39;n03888257&#39;], 10) . len(dataloaders[&#39;train&#39;]) . 148 . Check a a batch of our target values: . inputs, targets = next(iter(dataloaders[&#39;train&#39;])) . targets . tensor([1, 6, 3, 1, 6, 7, 8, 5, 0, 0, 1, 2, 6, 5, 3, 6, 1, 8, 1, 5, 6, 2, 8, 5, 5, 5, 7, 3, 9, 6, 9, 6, 4, 3, 4, 6, 3, 2, 4, 8, 4, 7, 5, 9, 2, 3, 4, 5, 6, 4, 8, 2, 0, 7, 5, 4, 9, 9, 6, 2, 5, 0, 3, 4]) . Visualize a batch of images . We can see below that there were some interpolation artifacts from the default PyTorch transforms: . def imshow(inp, title=None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.figure(figsize = (10,6)) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[&#39;train&#39;])) # Choose number items to display n_items = 4 # Make a grid from batch out = torchvision.utils.make_grid(inputs[:n_items]) imshow(out, title=[image_datasets[&#39;train&#39;].classes[x] for x in classes[:n_items]]) . Define Model . As per the fastai, we&#39;ll use a pretrained model as a feature extractor, by freezing it, adding a custom head and training that first. Then, we&#39;ll fine-tune the main pretrained network as well as the custom head. . As mentioned before, fastai implements an AdaptiveConcatPool2d layer, so to try to be as faithful as I could, I used the implementation below: . class AdaptiveConcatPool2d(nn.Module): &quot;Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`&quot; def __init__(self, size=None): super(AdaptiveConcatPool2d, self).__init__() self.size = size or 1 self.ap = nn.AdaptiveAvgPool2d(self.size) self.mp = nn.AdaptiveMaxPool2d(self.size) def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1) . I then combined the pretrained model, new custom head with AdaptiveConcatPool2d layer in the nn.Module. . In addition, I defined some freezing and unfreezing functions to allow a recreation for the staged-training method of fastai training then fine-tuning. The freeze function also avoids freezing the BatchNorm layers as per fastai. . We can see below the module in the create_model_body function how the pretrained network is cut at the final layers to allow for the placement of the new custom head, in the same way as fastai does. . The finally, we instantiate the complete model. . class transferNet(nn.Module): def __init__(self, model=None, num_classes=0): super(transferNet, self).__init__() # fastai equivalent pretrained body self.model_body = model # fastai equivalent head self.model_head = nn.Sequential(AdaptiveConcatPool2d(), nn.Flatten(), nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Dropout(p=0.25, inplace=False), nn.Linear(in_features=1024, out_features=512, bias=False), nn.ReLU(inplace=True), nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Dropout(p=0.5, inplace=False), nn.Linear(in_features=512, out_features=num_classes, bias=False), ) def freeze(self): for name, param in self.model_body.named_parameters(): if(&quot;bn&quot; not in name): param.requires_grad = False def unfreeze(self): for param in self.parameters(): param.requires_grad = True def forward(self, x): out = self.model_body(x) out = self.model_head(out) return out # create our the body of our model by cutting pretrained network def create_model_body(model, cut=-2): model_body_modules = list(model.children())[:cut] return nn.Sequential(*model_body_modules) model_body = create_model_body(model=torchvision.models.resnet18(pretrained=True)) # Now create our network net = transferNet(model=model_body, num_classes=num_classes) net . transferNet( (model_body): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (model_head): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=10, bias=False) ) ) . Training . Stage 1 - Base Model Frozen . Now that we&#39;ve defined our dataset, dataloader and transforms to be applied, we can define the training loop. . Now let&#39;s freeze our base network so that we are only training the head: . net.freeze() . We finally train for X epochs, ensuring we pass the same amount of epochs to the OneCycleLR scheduler. We evaluate classification accuracy every epoch. . def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print(f&#39;Epoch {epoch + 1}/{num_epochs}&#39;) print(&#39;-&#39; * 10) # Each epoch has a training and validation phase for phase in [&#39;train&#39;, &#39;val&#39;]: if phase == &#39;train&#39;: model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == &#39;train&#39;): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == &#39;train&#39;: loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == &#39;train&#39;: scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f&#39;{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}&#39;) # deep copy the model if phase == &#39;val&#39; and epoch_acc &gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print(f&#39;Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s&#39;) print(f&#39;Best val Acc: {best_acc:4f}&#39;) # load best model weights model.load_state_dict(best_model_wts) return model . Now let&#39;s define the following elements before we train: . the Loss Function - the model&#39;s method of determining how well it is predicting and what it will try to minimise (CrossEntropyLoss as per fastai&#39;s CrossEntropyLossFlat) | the Optimizer - what will optimise the loss function by gradient descent (Adam as per fastai) | A scheduler to step the learning rate as we train - in this case the PyTorch implementation of the one-cycle learning rate policy, with the same maximum learning rate as that defined in our fastai model. | . if cuda_available: net = net.cuda() # Now we define our loss function criterion criterion = nn.CrossEntropyLoss() # Define our optimizer by passing the network&#39;s parameters optimizer = optim.Adam(net.parameters()) # Define our scheduler and training num epochs for one-cycle policy num_epochs = 5 lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=3e-3, steps_per_epoch=len(dataloaders[&#39;train&#39;]), epochs=num_epochs) . And now we can train the head of our model: . model_conv = train_model(net, criterion, optimizer, lr_scheduler, num_epochs=num_epochs) . Epoch 1/5 - train Loss: 0.7206 Acc: 0.7834 val Loss: 0.1447 Acc: 0.9620 Epoch 2/5 - train Loss: 0.3490 Acc: 0.8944 val Loss: 0.1061 Acc: 0.9684 Epoch 3/5 - train Loss: 0.3110 Acc: 0.9033 val Loss: 0.0909 Acc: 0.9738 Epoch 4/5 - train Loss: 0.2875 Acc: 0.9095 val Loss: 0.0799 Acc: 0.9753 Epoch 5/5 - train Loss: 0.2733 Acc: 0.9153 val Loss: 0.0745 Acc: 0.9771 Training complete in 5m 34s Best val Acc: 0.977070 . Stage 2 - Unfreeze and train body . Now that we&#39;ve trained the head, we can unfreeze our pretrained base network and train that too. However, as per fastai, we&#39;ll try to use discriminative learning rates. . First, we&#39;ll unfreeze the network: . net.unfreeze() . Now we can set the discriminative learning rates. We&#39;ll need to define two separate optimizers and lr schedulers, for the parameters of the network we want to train with different rates - i.e. the pretrained body and the custom head: . optimizer = optim.Adam([{&quot;params&quot;: net.model_body.parameters()}, {&quot;params&quot;: net.model_head.parameters()}]) # Define our scheduler max_lr for one-cycle policy for the head and body separately in a list num_epochs = 5 lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=[3e-6, 3e-4], steps_per_epoch=len(dataloaders[&#39;train&#39;]), epochs=num_epochs) . model_conv = train_model(net, criterion, optimizer, lr_scheduler, num_epochs=num_epochs) . Epoch 1/5 - train Loss: 0.2694 Acc: 0.9174 val Loss: 0.0748 Acc: 0.9766 Epoch 2/5 - train Loss: 0.2739 Acc: 0.9142 val Loss: 0.0728 Acc: 0.9778 Epoch 3/5 - train Loss: 0.2688 Acc: 0.9167 val Loss: 0.0740 Acc: 0.9758 Epoch 4/5 - train Loss: 0.2469 Acc: 0.9254 val Loss: 0.0726 Acc: 0.9786 Epoch 5/5 - train Loss: 0.2625 Acc: 0.9190 val Loss: 0.0710 Acc: 0.9781 Training complete in 5m 53s Best val Acc: 0.978599 . Results . Using the straight PyTorch transfer learning method we acheived a final validation accuracy of: 97.9% that took around 11.5 minutes. Not bad, but not quite fastai standard! This 0.6% shortfall is likely due to the difference in the image transforms, which is definitely one of the main highlights of using of the fastai library. . In Part 2 of this blog series we&#39;ll simplify the PyTorch pipeline into Pytorch Lighting modules. .",
            "url": "https://mikful.github.io/blog/fastai/jupyter/pytorch/pytorch-lightning/2020/09/21/fastai-pytorch-lightning-part1.html",
            "relUrl": "/fastai/jupyter/pytorch/pytorch-lightning/2020/09/21/fastai-pytorch-lightning-part1.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Building an Orchid Genus Classifier App Using fastai, Render and Flutter - Part 2",
            "content": "In this blog series I&#39;ll show how to train an orchid genus classifier using fastai, deploy this to Render and create a Flutter app for the front-end. This will be done in two parts: . Dataset Collection and fastai Image Classifier Training | Render Deployment and Flutter App | This is part 2, please see the associated github repo for render and github repo for flutter for further implementation details. . Render Deployment . Now that we&#39;ve trained our model, we need to deploy our fastai Learner to a dockerized environment, such that we can perform inference on new images. The fastai course v3 had a starter package for Render that we&#39;ll update to work with the latest fastai library (version 2) and also for fastapi in this case. (Note: the fastbook and fastai website now give different options, such as a binder-hosted jupyter notebook for simple inference, or seemeai, although I find render and heroku are still very good options and more flexible than binder.) . requirements.txt . First, let&#39;s update our package dependencies (that we listed within our Notebook environment in Part 1 of this blog series). Be sure to use the CPU versions of PyTorch versions within the Notebook/Colab Environment for the docker deployment, not the larger Cuda enabled wheels as for the inference we do not require GPU usage. You can ensure to set these using +cpu. . torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html . These and all other dependencies in the requirements.txt file will be installed when the Dockerfile builds the container image using the following line: . RUN pip install --upgrade -r requirements.txt . server.py . Now we need to update the server.py file that contains the app. Firstly, the original Starlette asyncio library was replaced by the fastapi library, which is a little clearer in its syntax and easier to use. . Now we define the location of our exported fastai Learner. In part 1, we saw how to upload this to a Google Cloud Storage bucket - we need to ensure the permissions on the file are set such that we can download this, which can be done in the file settings in the bucket (for more details see here). . Then we set this within the script: . path = Path(__file__).parent export_file_url = &#39;https://storage.googleapis.com/fastai-export-bucket/export.pkl&#39; # google cloud bucket / file url export_file_name = &#39;export.pkl&#39; . Inference . The main functions within the server.py file are fairly self evident, in that it downloads the export.pkl file and then loads it into a new fastai Learner. . For the inference, the main function is contained within the following code section that takes the image bytes from the post request and performs the inference on it. The prediction is then returned in the JSON response. . @app.post(&quot;/analyze&quot;) async def analyze(file: bytes = File(...)): img_bytes = BytesIO(file) prediction, idx, preds = learn.predict(img_bytes.getvalue()) return JSONResponse({&#39;result&#39;: str(prediction)}) . HTML . Note that within the repo is also the css and html file for a webapp, that can be used as the interface to perform the inference if desired, instead of the Flutter app. However, in this case, I wanted to make a full Android app, not a webapp. . Flutter App . Flutter is a cross-platform app development solution, with near native application speeds. . Thanks to the code given at: https://github.com/dnmanveet/Fruit_classifier_app I could successfully connect my Render backend to a working Android Flutter App. . It was really as simple as changing the base render deployment location within main.dart. . String base = &quot;https://orchid-classifier.onrender.com&quot;; . Then, all that remained were some interface and styling tweaks and I had a pretty decent looking basic app for trying out on my own images! . &nbsp; . . Testing . Interestingly, in testing the predictions were initially way off, with many errors, however, I decided to try and zoom in on the images to see if the problem was related to the angle and distance of the photos. This resulted in a vast improvement - the model is clearly very sensitive to distance of the photo, or size of the flower relative to the image. This perhaps is unsurprising given that many of the images in the training data are professionally shot, resulting in well-centred and close-up images of the flowers in many cases, thereby containg much more information regarding the flower structure than an image shot further away, with little definition between the flowers. . &nbsp; . Original photo = incorrect prediction Zoomed photo = correct prediction &nbsp; . Original photo = incorrect prediction Zoomed photo = correct prediction &nbsp; . Original photo = incorrect prediction Zoomed photo = correct prediction &nbsp; . With photos taken face-on and focusing on the flowers the results were immediately of much higher accuracy: . . &nbsp; . As such, it will be necessary to build in a zoom function for cropping and centering the flower head before inference. In addition, on some rarer varieties of orchid species I tried there was a clear error, as the rarer species may not have been covered in the test data, however, I&#39;m surprised at how well this works in general and will be useful practical tool (with a bit of verification of course!). . Conclusion . All in all, this was a fun little project that appears to be giving quite decent results in the real world. Improvements and next steps for the app development, would be the following: . A zoom function to centre the flower head before the inference is made | Create an S3 storage bucket to store all the uploaded photos for further training of the model | Create an option for the user to confirm the prediction as correct or incorrect, and input the real flower genus if so, again for further training | . Another promising approach may be to use an architecture for more fine-grained feature recognition, such as the NTS-net described in the paper Learning to Navigate for Fine-grained Classification or a multi-modal network, that contains both descriptive text information and images. In addition, perhaps dimensional measurements would be useful for the classificaton process and would help to account for the very subtle differences between the genera. . I may update this blog series with a part 3 at another time including these items if I can incorporate them. . I hope this series may be of use to someone, and if anyone reading this has any queries regarding the training or deployment, or any comments in general, please let me know. .",
            "url": "https://mikful.github.io/blog/fastai/jupyter/render/flutter/2020/09/16/orchid-classifier-fastai-render-flutter-2.html",
            "relUrl": "/fastai/jupyter/render/flutter/2020/09/16/orchid-classifier-fastai-render-flutter-2.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Building an Orchid Genus Classifier App Using fastai, Render and Flutter - Part 1",
            "content": "In this blog series I&#39;ll show how to train an orchid genus classifier using fastai, deploy this to Render and create a Flutter app for the front-end. This will be done in two parts: . Dataset Collection and fastai Image Classifier Training | Render Deployment and Flutter App | This is part 1, please see the associated github repo for further implementation details. . Why Orchids? . I&#39;m lucky enough to live in the beautiful country of Colombia, where the scale and breadth of the natural world is as mind-blowing as you might expect and then some. Due to knowing an orchid enthusiast I&#39;m constantly encountering incredible orchids, but quickly forget the species. So I decided to give myself a little helping hand with this by creating an orchid image classifier. . Now, you may or may not know that orchids are the largest family of plants in the world, with approximately 30,000 species and another 120,000 hybrids. Quite a tall order for any image recognition system! So, I decided to set a more modest task of a classifier for genera, rather than species, and limit myself to only 20 of the most common genera to begin with as a proof of concept (maybe I&#39;ll attempt the full 150,000 species + hybrids later... perhaps). . Data Collection . Having been a fastai user for a while I&#39;ve been following along with the just released course and fastbook, both of which are fantastic. . Detailed within the book is a data collection method using Bing Image Search, however, I had already collected this dataset a while ago, but never fully created an app around it. For the dataset collection the Google Image Downloader was used within a Jupyter Notebook, downloading images of each genus into its own folder, which was a simple yet effective solution. For each genus, a maximum of 500 images were downloaded. The dataset was then cleaned with a duplicate image finder. . Bearing in mind that within a single genus there is a huge variety of features, sizes and colours, the hope was that there were enough common features of each genus for the CNN to be able to learn useful enough common representations for a classification system. So let&#39;s get on to training... . Training a fastai Image Classifier . Training the image classifier using fastai and transfer learning is a straightforward and speedy task, with very high accuracy, thanks to the bespoke image augmentations and state-of-the-art learner implementation available out-of-the-box. . !pip install --upgrade fastai -q . from fastai.vision.all import * . Datapath setup . path = Path(&quot;/content/dataset&quot;) . Check our data . fastai provides an image verficiation function, which can help us check our dataset before we get down to the training process and realise there are corrupted images. Thankfully, after cleaning the dataset using duplicate image finders and corrupt file finders, there were none. . fnames = get_image_files(path) fnames . (#6059) [Path(&#39;/content/dataset/Cymbidium/170.IMG_1049_new-1.jpg&#39;),Path(&#39;/content/dataset/Cymbidium/166.Cymbidium-Cymbidium-spp-Ficha-T%C3%A9cnica-Y-Cuidados.jpg&#39;),Path(&#39;/content/dataset/Cymbidium/319.Cymbidium_Mighty_Mouse_Minnie_x_Cymbidium_Lemon_Butter_Solana_Gold-830x557.jpg&#39;),Path(&#39;/content/dataset/Cymbidium/19.cymbidium_orchid.jpg&#39;),Path(&#39;/content/dataset/Cymbidium/251.CYMBIDIUM-ORCHID-RENES-WISH-wholesale-flowers.jpg&#39;),Path(&#39;/content/dataset/Cymbidium/287.orquidea-cymbidium-xl-848x477x80xX-1.jpg&#39;),Path(&#39;/content/dataset/Cymbidium/6.IMG_1793-e1538511932201.png&#39;),Path(&#39;/content/dataset/Cymbidium/283.Cymbidium-Sweetheart_d600.jpg&#39;),Path(&#39;/content/dataset/Cymbidium/10.cymbidium-orchid-600x600.jpg&#39;),Path(&#39;/content/dataset/Cymbidium/293.74707952-hermosa-orqu%C3%ADdea-cymbidium.jpg&#39;)...] . verify_images(fnames) . (#0) [] . Create our DataBlock and DataLoaders . A key element of the new multi-layered fastai api is the DataBlock, which is the template for the dataloader for passing data into the learner during the training stage. This combines the templates for the dataloader (item and label &quot;getters&quot;, training and validation splits, image augmentations and normalization). In addition, the CategoryBlock tells the learner to use Cross Entropy Loss (a combination of log-softmax and negative log-likelihood loss), for a single label classification. . In this case, I applied essentially the recommendations for training set from the fastai course and fastbook, i.e.: . Presizing - i.e. take a larger section of the image on the item transforms (which creates uniform image sizes before they are passed to the GPU for batch transformations) | the *aug_transforms which are a mixture of warps, brightness and colour changes and random resizing and cropping | Normalization by the ImageNet statistics as we are using a pretrained Resnet50, with transfer learning. | . On the validation set, by default a centre crop of the images will be taken. . orchids = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=RandomSplitter(valid_pct=0.2, seed=42), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), # Presizing batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) . Having done that, we can instantiate our dataloaders (also defining the batch size) and also show a batch of very pretty images. . dls = orchids.dataloaders(path, bs=64) . dls.show_batch(max_n=9) . Create our Learner . Now we create our Learner, which handles the training and validation loops. In this case, we&#39;ll use a pretrained ResNet-50 from the PyTorch model zoo. The cnn_learner automatically cuts the final fully connected layers of the pretrained network (which have the original label outputs) and applies a new custom head network to the main pretrained body, with the correct amount of outputs, based on the number of labels in the dataset. . learn = cnn_learner(dls, resnet50, pretrained=True, loss_func=CrossEntropyLossFlat(), opt_func=Adam, metrics=accuracy) . Find our Learning Rate . A key element to training a neural network quickly and to high accuracy is having an optimum learning rate for the loss function optimizer (in this case Adam) at the right time during training. fastai makes choosing this learning rate and instantiating a &quot;one-cycle&quot; learning rate very easy (i.e. a single cycle over the course of the entire training, which starts slowly, ramps up to a maximum rate and then reduces in a cosine annealing fashion to zero). In addition, this inversely cycles the momentum of the Adam optimizer. . learn.lr_find(suggestions=True) . /usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data. Expecting to read 2 bytes but only got 0. warnings.warn(str(msg)) . SuggestedLRs(lr_min=0.005754399299621582, lr_steep=0.0010000000474974513) . Train Our Model . Now we&#39;ve found a good learning rate, we can train the head of our model. At this point the pretrained model&#39;s parameters are still frozen, other than the Batch Norm layers, which allows the pretrained network to act as a fixed-feature extractor for the new custom head. . Train head . learn.fit_one_cycle(10, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.845518 | 1.423715 | 0.599505 | 01:24 | . 1 | 1.730574 | 1.151923 | 0.693642 | 01:23 | . 2 | 1.225178 | 0.995854 | 0.725021 | 01:23 | . 3 | 0.976378 | 0.919860 | 0.746490 | 01:24 | . 4 | 0.751894 | 0.834673 | 0.772915 | 01:24 | . 5 | 0.610444 | 0.807859 | 0.784476 | 01:25 | . 6 | 0.491962 | 0.770431 | 0.780347 | 01:25 | . 7 | 0.416870 | 0.768377 | 0.791907 | 01:26 | . 8 | 0.369887 | 0.764954 | 0.796036 | 01:27 | . 9 | 0.346570 | 0.751820 | 0.799339 | 01:27 | . Unfreeze and train base layers . An accuracy of 80% is not bad for 10 epochs, given the variance of the types of species within each genus! Not bad at all. Now let&#39;s unfreeze the base model parameters and train the full network. . To do this we also need to find our new learning rate. . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=2.290867705596611e-05, lr_steep=1.3182567499825382e-06) . Now that we&#39;ve got some idea of our new learning rate, we&#39;ll apply this using discriminative layer learning. This refers to using a lower learning rate for the base network parameters and a higher learning rate for the custom head, such that we don&#39;t &quot;damage&quot; the pre-existing parameters trained for many hours on ImageNet. . learn.fit_one_cycle(20, slice=(6e-6,6e-5)) . epoch train_loss valid_loss accuracy time . 0 | 0.453058 | 0.840666 | 0.772915 | 01:29 | . 1 | 0.651639 | 1.414684 | 0.667217 | 01:28 | . 2 | 1.032905 | 1.770493 | 0.614368 | 01:29 | . 3 | 1.065532 | 2.119179 | 0.496284 | 01:29 | . 4 | 1.099497 | 1.405346 | 0.620974 | 01:28 | . 5 | 0.964336 | 1.601728 | 0.587118 | 01:29 | . 6 | 0.923180 | 1.652808 | 0.584641 | 01:29 | . 7 | 0.867319 | 0.970786 | 0.728324 | 01:28 | . 8 | 0.805006 | 1.195390 | 0.651528 | 01:29 | . 9 | 0.647579 | 1.135085 | 0.698596 | 01:30 | . 10 | 0.591589 | 1.076413 | 0.725846 | 01:30 | . 11 | 0.526392 | 1.291697 | 0.700248 | 01:31 | . 12 | 0.432536 | 0.782687 | 0.788604 | 01:30 | . 13 | 0.302862 | 0.750325 | 0.793559 | 01:33 | . 14 | 0.241066 | 0.691607 | 0.802642 | 01:33 | . 15 | 0.177758 | 0.689294 | 0.831544 | 01:32 | . 16 | 0.130680 | 0.665458 | 0.830718 | 01:32 | . 17 | 0.100354 | 0.668383 | 0.829893 | 01:31 | . 18 | 0.092311 | 0.662804 | 0.829067 | 01:31 | . 19 | 0.073622 | 0.668779 | 0.829067 | 01:30 | . A final model performance of 83% is quite impressive! . Plot Confusion Matrix . We can also view our actual vs. predicted labels from the output of the validation set of the model. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . Although we can see that Miltonia is commonly confused with Miltoniopsis (a little unsurpising, given that they were commonly lumped together by botanists until the 1970s), this is a little tricky to read given the amount of labels, so let&#39;s plot the highest losses. . We can see here that the losses are very high indeed for the errors in some cases, however, these could well be hybrids of the two genera - fortunately I know an orchid expert who can help me refine this model and perhaps I could turn it into a multi-label classifier instead of single label. For now though, we&#39;ll go deploy it, and try it in the real world on some new images and refine later. . interp.plot_top_losses(4, nrows=2, figsize=(10,10)) . Export and upload the learner to a Google Cloud Storage Bucket for the app . The next stage is to export our Learner, such that we can use it in the app for predictions. We&#39;ll also upload this to a Google Cloud Storage Bucket, such that the app script can access and download it during deployment. . learn.export() . from google.colab import auth auth.authenticate_user() # set project id project_id = &#39;project-id&#39; !gcloud config set project {project_id} # bucket name bucket_name = &#39;project-bucket-name&#39; # copy to our bucket !gsutil cp &quot;/content/export.pkl&quot; gs://{bucket_name}/ . Updated property [core/project]. Copying file:///content/export.pkl [Content-Type=application/octet-stream]... Operation completed over 1 objects/98.9 MiB. . List dependencies for Render deployment . Another critical piece of deploying an app are the dependencies - we need to make sure we&#39;re deploying the Docker container with the same library dependencies that we trained the app on. These are straightforward to see within your Notebook environment. . !pip list . Package Version -- absl-py 0.10.0 alabaster 0.7.12 albumentations 0.1.12 altair 4.1.0 argon2-cffi 20.1.0 asgiref 3.2.10 astor 0.8.1 astropy 4.0.1.post1 astunparse 1.6.3 async-generator 1.10 atari-py 0.2.6 atomicwrites 1.4.0 attrs 20.2.0 audioread 2.1.8 autograd 1.3 Babel 2.8.0 backcall 0.2.0 beautifulsoup4 4.6.3 bleach 3.1.5 blis 0.4.1 bokeh 2.1.1 boto 2.49.0 boto3 1.14.59 botocore 1.17.59 Bottleneck 1.3.2 branca 0.4.1 bs4 0.0.1 CacheControl 0.12.6 cachetools 4.1.1 catalogue 1.0.0 certifi 2020.6.20 cffi 1.14.2 chainer 7.4.0 chardet 3.0.4 click 7.1.2 cloudpickle 1.3.0 cmake 3.12.0 cmdstanpy 0.9.5 colorlover 0.3.0 community 1.0.0b1 contextlib2 0.5.5 convertdate 2.2.2 coverage 3.7.1 coveralls 0.5 crcmod 1.7 cufflinks 0.17.3 cupy-cuda101 7.4.0 cvxopt 1.2.5 cvxpy 1.0.31 cycler 0.10.0 cymem 2.0.3 Cython 0.29.21 daft 0.0.4 dask 2.12.0 dataclasses 0.7 datascience 0.10.6 debugpy 1.0.0rc2 decorator 4.4.2 defusedxml 0.6.0 descartes 1.1.0 dill 0.3.2 distributed 1.25.3 Django 3.1.1 dlib 19.18.0 dm-tree 0.1.5 docopt 0.6.2 docutils 0.15.2 dopamine-rl 1.0.5 earthengine-api 0.1.234 easydict 1.9 ecos 2.0.7.post1 editdistance 0.5.3 en-core-web-sm 2.2.5 entrypoints 0.3 ephem 3.7.7.1 et-xmlfile 1.0.1 fa2 0.3.5 fancyimpute 0.4.3 fastai 2.0.11 fastcore 1.0.9 fastdtw 0.3.4 fastprogress 1.0.0 fastrlock 0.5 fastscript 1.0.0 fbprophet 0.7.1 feather-format 0.4.1 filelock 3.0.12 firebase-admin 4.1.0 fix-yahoo-finance 0.0.22 Flask 1.1.2 folium 0.8.3 future 0.16.0 gast 0.3.3 GDAL 2.2.2 gdown 3.6.4 gensim 3.6.0 geographiclib 1.50 geopy 1.17.0 gin-config 0.3.0 glob2 0.7 google 2.0.3 google-api-core 1.16.0 google-api-python-client 1.7.12 google-auth 1.17.2 google-auth-httplib2 0.0.4 google-auth-oauthlib 0.4.1 google-cloud-bigquery 1.21.0 google-cloud-core 1.0.3 google-cloud-datastore 1.8.0 google-cloud-firestore 1.7.0 google-cloud-language 1.2.0 google-cloud-storage 1.18.1 google-cloud-translate 1.5.0 google-colab 1.0.0 google-pasta 0.2.0 google-resumable-media 0.4.1 googleapis-common-protos 1.52.0 googledrivedownloader 0.4 graphviz 0.10.1 grpcio 1.32.0 gspread 3.0.1 gspread-dataframe 3.0.8 gym 0.17.2 h5py 2.10.0 HeapDict 1.0.1 holidays 0.10.3 holoviews 1.13.3 html5lib 1.0.1 httpimport 0.5.18 httplib2 0.17.4 httplib2shim 0.0.3 humanize 0.5.1 hyperopt 0.1.2 ideep4py 2.0.0.post3 idna 2.10 image 1.5.32 imageio 2.4.1 imagesize 1.2.0 imbalanced-learn 0.4.3 imblearn 0.0 imgaug 0.2.9 importlib-metadata 1.7.0 imutils 0.5.3 inflect 2.1.0 iniconfig 1.0.1 intel-openmp 2020.0.133 intervaltree 2.1.0 ipykernel 4.10.1 ipython 5.5.0 ipython-genutils 0.2.0 ipython-sql 0.3.9 ipywidgets 7.5.1 itsdangerous 1.1.0 jax 0.1.75 jaxlib 0.1.52 jdcal 1.4.1 jedi 0.17.2 jieba 0.42.1 Jinja2 2.11.2 jmespath 0.10.0 joblib 0.16.0 jpeg4py 0.1.4 jsonschema 2.6.0 jupyter 1.0.0 jupyter-client 5.3.5 jupyter-console 5.2.0 jupyter-core 4.6.3 jupyterlab-pygments 0.1.1 kaggle 1.5.8 kapre 0.1.3.1 Keras 2.4.3 Keras-Preprocessing 1.1.2 keras-vis 0.4.1 kiwisolver 1.2.0 knnimpute 0.1.0 korean-lunar-calendar 0.2.1 librosa 0.6.3 lightgbm 2.2.3 llvmlite 0.31.0 lmdb 0.99 lucid 0.3.8 LunarCalendar 0.0.9 lxml 4.2.6 Markdown 3.2.2 MarkupSafe 1.1.1 matplotlib 3.2.2 matplotlib-venn 0.11.5 missingno 0.4.2 mistune 0.8.4 mizani 0.6.0 mkl 2019.0 mlxtend 0.14.0 more-itertools 8.5.0 moviepy 0.2.3.5 mpmath 1.1.0 msgpack 1.0.0 multiprocess 0.70.10 multitasking 0.0.9 murmurhash 1.0.2 music21 5.5.0 natsort 5.5.0 nbclient 0.5.0 nbconvert 5.6.1 nbdev 1.0.18 nbformat 5.0.7 nest-asyncio 1.4.0 networkx 2.5 nibabel 3.0.2 nltk 3.2.5 notebook 5.3.1 np-utils 0.5.12.1 numba 0.48.0 numexpr 2.7.1 numpy 1.18.5 nvidia-ml-py3 7.352.0 oauth2client 4.1.3 oauthlib 3.1.0 okgrade 0.4.3 opencv-contrib-python 4.1.2.30 opencv-python 4.1.2.30 openpyxl 2.5.9 opt-einsum 3.3.0 osqp 0.6.1 packaging 20.4 palettable 3.3.0 pandas 1.0.5 pandas-datareader 0.8.1 pandas-gbq 0.11.0 pandas-profiling 1.4.1 pandocfilters 1.4.2 panel 0.9.7 param 1.9.3 parso 0.7.1 pathlib 1.0.1 patsy 0.5.1 pexpect 4.8.0 pickleshare 0.7.5 piexif 1.1.3 Pillow 7.0.0 pip 19.3.1 pip-tools 4.5.1 plac 1.1.3 plotly 4.4.1 plotnine 0.6.0 pluggy 0.7.1 portpicker 1.3.1 prefetch-generator 1.0.1 preshed 3.0.2 prettytable 0.7.2 progressbar2 3.38.0 prometheus-client 0.8.0 promise 2.3 prompt-toolkit 1.0.18 protobuf 3.12.4 psutil 5.4.8 psycopg2 2.7.6.1 ptyprocess 0.6.0 py 1.9.0 pyarrow 0.14.1 pyasn1 0.4.8 pyasn1-modules 0.2.8 pycocotools 2.0.2 pycparser 2.20 pyct 0.4.7 pydata-google-auth 1.1.0 pydot 1.3.0 pydot-ng 2.0.0 pydotplus 2.0.2 PyDrive 1.3.1 pyemd 0.5.1 pyglet 1.5.0 Pygments 2.6.1 pygobject 3.26.1 pymc3 3.7 PyMeeus 0.3.7 pymongo 3.11.0 pymystem3 0.2.0 PyOpenGL 3.1.5 pyparsing 2.4.7 pyrsistent 0.16.0 pysndfile 1.3.8 PySocks 1.7.1 pystan 2.19.1.1 pytest 3.6.4 python-apt 1.6.5+ubuntu0.3 python-chess 0.23.11 python-dateutil 2.8.1 python-louvain 0.14 python-slugify 4.0.1 python-utils 2.4.0 pytz 2018.9 pyviz-comms 0.7.6 PyWavelets 1.1.1 PyYAML 3.13 pyzmq 19.0.2 qtconsole 4.7.7 QtPy 1.9.0 regex 2019.12.20 requests 2.23.0 requests-oauthlib 1.3.0 resampy 0.2.2 retrying 1.3.3 rpy2 3.2.7 rsa 4.6 s3transfer 0.3.3 scikit-image 0.16.2 scikit-learn 0.22.2.post1 scipy 1.4.1 screen-resolution-extra 0.0.0 scs 2.1.2 seaborn 0.10.1 Send2Trash 1.5.0 setuptools 50.3.0 setuptools-git 1.2 Shapely 1.7.1 simplegeneric 0.8.1 six 1.15.0 sklearn 0.0 sklearn-pandas 1.8.0 slugify 0.0.1 smart-open 2.1.1 snowballstemmer 2.0.0 sortedcontainers 2.2.2 spacy 2.2.4 Sphinx 1.8.5 sphinxcontrib-serializinghtml 1.1.4 sphinxcontrib-websupport 1.2.4 SQLAlchemy 1.3.19 sqlparse 0.3.1 srsly 1.0.2 statsmodels 0.10.2 sympy 1.1.1 tables 3.4.4 tabulate 0.8.7 tblib 1.7.0 tensorboard 2.3.0 tensorboard-plugin-wit 1.7.0 tensorboardcolab 0.0.22 tensorflow 2.3.0 tensorflow-addons 0.8.3 tensorflow-datasets 2.1.0 tensorflow-estimator 2.3.0 tensorflow-gcs-config 2.3.0 tensorflow-hub 0.9.0 tensorflow-metadata 0.24.0 tensorflow-privacy 0.2.2 tensorflow-probability 0.11.0 termcolor 1.1.0 terminado 0.8.3 testpath 0.4.4 text-unidecode 1.3 textblob 0.15.3 textgenrnn 1.4.1 Theano 1.0.5 thinc 7.4.0 tifffile 2020.9.3 toml 0.10.1 toolz 0.10.0 torch 1.6.0+cu101 torchsummary 1.5.1 torchtext 0.3.1 torchvision 0.7.0+cu101 tornado 5.1.1 tqdm 4.41.1 traitlets 4.3.3 tweepy 3.6.0 typeguard 2.7.1 typing-extensions 3.7.4.3 tzlocal 1.5.1 umap-learn 0.4.6 uritemplate 3.0.1 urllib3 1.24.3 vega-datasets 0.8.0 wasabi 0.8.0 wcwidth 0.2.5 webencodings 0.5.1 Werkzeug 1.0.1 wheel 0.35.1 widgetsnbextension 3.5.1 wordcloud 1.5.0 wrapt 1.12.1 xarray 0.15.1 xgboost 0.90 xkit 0.0.0 xlrd 1.1.0 xlwt 1.3.0 yellowbrick 0.9.1 zict 2.0.0 zipp 3.1.0 . And let&#39;s not forget the Python version for information. . !python --version . Python 3.6.9 . Next Up . Now that we&#39;ve trained what appears to be a well performing model, let&#39;s get on to the deployment to render and creating the front-end flutter app in Part 2. .",
            "url": "https://mikful.github.io/blog/fastai/jupyter/render/flutter/2020/09/15/orchid-classifier-fastai-render-flutter-1.html",
            "relUrl": "/fastai/jupyter/render/flutter/2020/09/15/orchid-classifier-fastai-render-flutter-1.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 3 - Results and Analysis",
            "content": "Welcome to part 3 of a blog series based on my Udacity Machine Learning Engineer Nanodegree Capstone project. This section defines the model implementation using the in-development fastai2 audio library and Google Cloud AI Platform notebooks. . The blog series is structured as follows, please follow the links for other sections: . Problem Definition, Proposed Solution and Data Exploration | Methodology and Implementation | Results and Analysis | Links will be provided as the series progresses. Please see the associated GitHub repository for all notebooks. . A huge thanks goes to fastai and the fastai2 audio contributors for their amazing work. . IV. Results . Model Evaluation and Validation . The procedures outlined in the above sections were used to obtain a final prediction score of 0.69788. . . ​ Fig 13. Final Prediction Score, lwl-rap . Justification . The Competition baseline score of 0.53792 was beaten by a considerable margin of 16%. The winning score of 0.75980 was not achieved, with a shortfall of 6%, however, the winning model and the top-scoring models documented 1 23 used training stages of 3x to 5x as long as the one implemented herein. . Additionally, the amount of pre-processing within the solution presented was negligible, afforded by the very impressive fastai2 audio library, whereas other solutions involved the significant extra time and storage usage of conversion from audio files to spectrogram images . As such, it is considered that the model performance is more than satisfactory for the task of auto-tagging audio files and further testing would be needed to see if it could achieve the same performance as the top-scorers. . V. Conclusion . Free-Form Visualization . One very clear visual element of the datasets is the difference in the noise level and quality of the recordings between the two datasets. This is shown clearly below: . . ​ Fig 14. Differences in noise level between Curated and Noisy Set . The impact of this was that, when using the noisy dataset for training alone, the lwl-rap score was approximately 20% lower than using only the curated dataset for training. As such, the two stage training method was used for bigger performance gains. . Reflection . The stages involved in the production of this model were varied, each step requiring careful consideration of the data and audio file to spectrogram augmentations. . The most interesting aspects of the project were considered to be the time/performance cost balance required in the data augmentations’ effect on the training. It was extremely interesting to also see the stark impact of the K-Folds Validation training procedure and Test-Time-Augmentations on the final score. . The main difficulties in the project were the balancing of trying to achieve a good score while minimising training and iteration costs on the Google Cloud Platform. Given the nature of the problem, the training times were necessarily long and this was not anticipated fully at the outset of choosing the problem set. In a production setting this would be a significant consideration to bear in mind. . The final model is considered to be of good performance and will serve to inform future model development for production-stage inference of audio auto-tagging. Naturally, the TTA and ensembling of predictions means this model should only be used in an offline scenario for complex and noisy audio tagging, however, it should be noted that using this system with a standard multi-label accuracy metric, a score of over 95% was achieved within the first few epochs of training on both the Curated and Noisy Train sets. . As such, a similar approach, perhaps using a different metric (standard multi-label accuracy), could be used for a deployed inference model to be used within applications for audio classification e.g. for bird sounds, which is an area of interest of the author. . Improvement . It is considered the final prediction score could be improved in a number of ways: . Longer training times | Further fine-tuning of the mel-spectrogram settings | further ensembling of models | further data engineering, such as further training on correctly predicted noisy data 2 | . The majority of the techniques were implemented within the development of the model, however, in the best performing models in the competition, more advanced data engineering was undertaken such as cross-referencing prediction scores between models and only doing further training on the audio files that were correctly selected over a certain threshold across models. This would require significant extra development that the author did not achieve within the time frame available. . Conclusion . That concludes the 3 part series on my Udacity Machine Learning Engineer Nanodegree Capstone project. It was a fantastic learning experience and I hope one that is also useful for others. . If you have any questions or feedback about this post, I’d be very happy to hear them. Please contact me at my GitHub or Twitter using the links below. . References: . https://github.com/lRomul/argus-freesound &#8617; . | https://github.com/ebouteillon/freesound-audio-tagging-2019 &#8617; &#8617;2 . | https://medium.com/@mnpinto/multi-label-audio-classification-7th-place-public-lb-solution-for-freesound-audio-tagging-2019-a7ccc0e0a02f &#8617; . |",
            "url": "https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/19/Udacity-Capstone-Part-3-Results-and-Analysis.html",
            "relUrl": "/deep%20learning/fastai2/audio/markdown/2020/06/19/Udacity-Capstone-Part-3-Results-and-Analysis.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 2 - Methodology and Implementation",
            "content": "Welcome to Part 2 of a blog series based on my Udacity Machine Learning Engineer Nanodegree Capstone project. This section defines the model implementation using the in-development fastai2 audio library and Google Cloud AI Platform notebooks. . The blog series is structured as follows, please follow the links for other sections: . Problem Definition, Analysis, Methods and Algorithms | Methodology and Implementation | Results and Analysis | Links will be provided as the series progresses. Please see the associated GitHub repository for all notebooks. . A huge thanks goes to fastai and the fastai2 audio contributors for their amazing work. . &nbsp; . IV. Methodology and Implementation . Data Pre-processing . Fastai2 Audio . The remarkable in-development fastai2 audio package was used to convert the audio files into mel-spectrogram 2D tensors on-the-fly, as a form of highly efficient data processing, rather than pre-processing and having to save spectrograms to a different dataset. This was done using the following process: . Create Pandas DataFrames for the files, suing the train_curated.csv and train_noisy.csv files provided by the competition, removing corrupted or empty files as given in the competition guidance: | def create_train_curated_df(file, remove_files=[]): df_curated = pd.read_csv(file) df_curated[&#39;fname&#39;] = &#39;../data/train_curated/&#39; + df_curated[&#39;fname&#39;] df_curated.set_index(&#39;fname&#39;, inplace=True) df_curated.loc[remove_files] df_curated.drop(index=remove_files, inplace=True) df_curated.reset_index(inplace=True) return df_curated def create_train_noisy_df(file): df_noisy = pd.read_csv(file) df_noisy[&#39;fname&#39;] = &#39;../data/train_noisy/&#39; + df_noisy[&#39;fname&#39;] return df_noisy # Create Curated training set df # Remove corrupt and empty files as per Kaggle guidance remove_files = [&#39;f76181c4.wav&#39;, &#39;77b925c2.wav&#39;, &#39;6a1f682a.wav&#39;, &#39;c7db12aa.wav&#39;, &#39;7752cc8a.wav&#39;, &#39;1d44b0bd.wav&#39;] remove_files = [&#39;../data/train_curated/&#39; + i for i in remove_files] df_curated = create_train_curated_df(&#39;../data/train_curated.csv&#39;, remove_files=remove_files) df_curated.head() . The DataFrames were then used to supply the fastai DataBlock API with the filenames, which could then be processed using the fastai2 audio item_transformations which are applied to each file before training. After significant testing iterations the audio transformation settings were chosen as follows: . DBMelSpec = SpectrogramTransformer(mel=True, to_db=True) # convert to Mel-spectrograms clip_length = 2 # clip subsection length in seconds sr = 44100 # sample rate f_min = 20 # mel-spectrogram minimum frequency f_max = 20000 # mel-spectrogram minimum frequency n_mels = 128 # mel-frequency bins, dictates the y-axis pixel size hop_length = math.ceil((clip_length*sr)/n_mels)# determines width of image. for square to match n_mels, set math.ceil((clip_length*sr)/n_mels) nfft = n_mels * 20 # = 2560 for higher resolution in y-axis win_length = 1024 # sample windowing top_db = 60 # highest noise level in relative db . The top_db parameter setting of 60dB was important, as the noisy train set had high background noise (low signal-to-noise ratio) which with a higher setting lead to obscured features in the mel-spectrograms. | . In addition to the mel-spectrogram settings above, the following additional item transformations were undertaken: . RemoveSilence - Splits the original signal at points of silence more than 2 * pad_ms . | CropSignal - Crops a signal by clip_length seconds and adds zero-padding by default if the signal is less than clip_length . | aud2spec - The mel-spectrogram settings from above . | MaskTime - Uses Google’s SpecAugment1 time masking procedure to zero-out time domain information as a form of data augmentation . | MaskFreq - Uses Google’s SpecAugment1 frequency masking procedure to zero-out frequency domain information as a form of data augmentation . | . item_tfms = [RemoveSilence(threshold=20), CropSignal(clip_length*1000), aud2spec, MaskTime(num_masks=1, size=8), MaskFreq(num_masks=1, size=8)] . Batch Transforms . In addition to the item transforms above, Batch Transforms were used as part of the DataBlock API, which are transformations applied per batch during training: . Normalize() - normalizes the data taking a single batch’s statistics | RatioResize(256) - during training (other than the first 10 epochs of the noisy data for speed), the mel-spectrogram tensors were resized from 128x128px to 256x256px through bilinear interpolation as this has been shown to give gains in performance over simply creating a 256x256 tensor from the outset. | Brightness and Contrast augmentations were also applied in the training cycles to improve performance | . batch_tfms = [Normalize(), RatioResize(256), Brightness(max_lighting=0.2, p=0.75), Contrast(max_lighting=0.2, p=0.75)] . No further augmentations were applied as would otherwise be typical in many image classification processes. Many typical image augmentations applied are not suitable for spectrogram representations of audio, for example, cropping/warping/rotating the spectrogram tensor would warp the relative frequency relationship and thus, would not gain any benefit at testing time. This was found to be true during tests of various transforms. . The above augmentations (prior to batch transformations), produced the following mel-spectrograms as 2D tensors (note the time and frequency masking augmentations): . &nbsp; . . Fig 8. Augmented Mel-spectrograms . &nbsp; . Implementation . The data augmentations stated above were used to significantly improve the performance of the classifier during the following K-Fold training cycles. . The implemented training method was chosen based on the Competitions 6th place winner’s technique1, however, only the first two stages were implemented as follows due to the cost requirements using GCP: . &nbsp; . . Fig 9. Train-Test-Prediction Stages . &nbsp; . Stage 1 - Noisy Training Set . As can be seen below, the following 5-Fold training cycle was used on the noisy set. The indices of the DataFrame were shuffled to ensure the data splits were chosen at random, but without overlap using SKLearn’s k-Folds module. The cycle began with 10 epochs of training at a higher learning rate and then 10 epochs of training at a lower learning rate (set after using fastai’s learning rate finder during the testing stage) used to fine-tune the model’s weights further. Please see the associated Jupyter Notebook for the training output. . The models were then saved for further training on the curated training set. . Note: No MixUp augmentations were used on the Noisy Training set. . from sklearn.model_selection import KFold # Declare Number Folds n_splits = 5 kf = KFold(n_splits=n_splits, random_state=42, shuffle=True) # random_state for repeatable results, shuffle indices df = df_noisy # to use random subset, use df = df_.sample(frac=0.5, replace=False, random_state=1) # take random subset of the noisy dataframe for faster training (otherwise need 6.5 hours for all folds with complete dataset) for fold, (train_idx, valid_idx) in enumerate(kf.split(df)): print(f&#39; nNoisy Train Set - Fold {fold+1}/{n_splits}&#39;) def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39;,&#39;) # split labels on &#39;,&#39; def get_dls(train_cycle): if train_cycle == 1: batch_tfms = [Normalize(), Brightness(max_lighting=0.2, p=0.75), Contrast(max_lighting=0.2, p=0.75)] elif train_cycle == 2: batch_tfms = [Normalize(), RatioResize(256), # progressive resize to 256x256px Brightness(max_lighting=0.2, p=0.75), Contrast(max_lighting=0.2, p=0.75)] dblock = DataBlock(blocks=(AudioBlock, MultiCategoryBlock), splitter=IndexSplitter(valid_idx), # split using df index get_x=get_x, get_y=get_y, item_tfms = item_tfms, batch_tfms = batch_tfms ) return dblock.dataloaders(df, bs=64) dls = get_dls(train_cycle=1) dls.show_batch(max_n=6) model = xresnet50(pretrained=False, act_cls=Mish, sa=True, c_in=1, n_out=80) #create custom xresnet: 1 input channel, 80 output nodes, self-attention, Mish activation function model = convert_MP_to_blurMP(model, nn.MaxPool2d) # convert MaxPool2D layers to MaxBlurPool learn = Learner(dls, model=model, loss_func=BCEWithLogitsLossFlat(), opt_func = ranger, metrics=[lwlrap]) # pass custom model to Learner, no mixup for noisy set as fewer epochs learn.fit_flat_cos(10, lr=3e-3) print(&#39;Batch transforming images to 256x256px and training further.&#39;) dls = get_dls(train_cycle=2) learn.dls = dls learn.fit_flat_cos(10, lr=3e-3/3) print(&#39;Saving Learner...&#39;) learn.save(f&#39;stage-1_noisy_fold-{fold+1}_sota2&#39;) . Stage 2 - Curated Train Set . After all 5 models had been trained on the Noisy set, the models were then trained on different 5-folds of the Curated Set. This essentially gave 5 distinct models, all trained on different data for later ensembling. . Note: MixUp data augmentations were applied to the Curated Train set, shown as training callback below. This is whereby two spectrogram tensors are combined into a single 2D tensor with a certain percentage blend (50% in this case), allowing the network to learn double the amount of features and labels per batch. This also provides a form of regularization for the model which improves generalization on the validation/test sets. . ## K-Folds training loop df = df_curated for fold, (train_idx, valid_idx) in enumerate(kf.split(df)): print(f&#39; nCurated Train Set - Fold {fold+1}/{n_splits}&#39;) def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39;,&#39;) # split labels on &#39;,&#39; dblock = DataBlock(blocks=(AudioBlock, MultiCategoryBlock), splitter=IndexSplitter(valid_idx), # split using df index get_x=get_x, get_y=get_y, item_tfms = item_tfms, batch_tfms = batch_tfms # including RatioResize(256) ) dls = dblock.dataloaders(df, bs=64) dls.show_batch(max_n=3) print(f&#39; nLoading Stage 1 model - fold {fold+1}.&#39;) model = xresnet50(pretrained=False, act_cls=Mish, sa=True, c_in=1, n_out=80) #create custom xresnet: 1 input channel, 80 output nodes, self-attention, Mish activation function model = convert_MP_to_blurMP(model, nn.MaxPool2d) # convert MaxPool2D layers to MaxBlurPool learn = Learner(dls, model=model, loss_func=BCEWithLogitsLossFlat(), opt_func=ranger, metrics=[lwlrap]) # pass custom model to Learner, no mixup for noisy set as fewer epochs learn.load(f&#39;stage-1_noisy_fold-{fold+1}_sota2&#39;) learn.dls = dls learn.add_cb(MixUp()) # add mixup callback print(&#39; nTraining on Curated Set:&#39;) learn.fit_flat_cos(50, 3e-4) print(&#39;Saving model...&#39;) learn.save(f&#39;stage-2_curated_fold-{fold+1}_sota2&#39;) . Testing . At the testing stage, Test-Time-Augmentations and ensembling the predictions of all 5 different Stage-2 models were used to improve the final predictions. . # grab test filenames from submission csv df_fnames = pd.read_csv(&#39;../data/sample_submission.csv&#39;) fnames = df_fnames.fname df_fnames = &#39;../data/test/&#39; + df_fnames.fname print(df_fnames[:5]) # get predictions for fold in range(n_splits): stage = 2 print(f&#39;Getting predictions from stage {stage} fold {fold+1} model.&#39;) learn = learn.load(f&#39;stage-2_curated_fold-{fold+1}_sota2&#39;) dl = learn.dls.test_dl(df_fnames) # predict using tta preds, targs = learn.tta(dl=dl) preds = preds.cpu().numpy() if fold == 0: predictions = preds else: predictions += preds # Average predictions predictions /= n_splits # Create Submission DataFrame df_sub = pd.DataFrame(predictions) df_sub.columns = learn.dls.vocab df_sub.insert(0, &quot;fname&quot;, 0) df_sub.fname = fnames df_sub.head() . The produced .csv file was then submitted to Kaggle. . Refinement . Initial Model . The initial CNN architecture used was a pre-trained (on ImageNet) xresnet50 model for speed of iteration. This was trained on a single fold smaller subset of the Noisy data (ranging from 20-80% using the DataBlock API - RandomSubsetSplitter()function) used for faster iteration on the noisy subset, while all of the Curated data was used. The data augmentation settings were slightly different however, as non-square mel-spectrograms were used to see if larger spectrograms could give improved scores, which was the case, however, this was at the expense of training time. . This highest score achieved by any initial model, was an lwl-rap of 0.61013 on the test-set: . &nbsp; . . Fig 10. Initial Best Score . &nbsp; . This score, while not bad for a small amount of testing and still beating the competition baseline, was far from achieving near state-of-the-art performance. . Test-Time-Augmentation was shown to provide a benefit of &gt;3% improvement during further testing rounds using a single training fold on the noisy and curated datasets, shown as the top score in the following image: . &nbsp; . . Fig 11. Improvement using TTA . &nbsp; . After reading further the writeups of the competition winners and high scorers 234, it was decided that a K-Folds validation approach was required in order to substantially improve the performance. . In addition, due to the large size of the spectrograms in the initial testing phase that would cause extremely slow training over so many folds (a 5x increase in epochs), these were replaced with smaller 128x128px (using 128 mel-bins and the settings shown in the above Data Pre-processing section). It was decided to first try training for a single fold on the Noisy set (90%/10% train/test split) and then split this model into 5 separate models for the further training on the Curated Set. . What’s more, the pretrained xresnet50 model was replaced by the state-of-the-art xresnet50 model described in the previous Section II: Algorithms and Techniques. This was in line with the allowance of only non-pretrained models in the competition and was also shown to provide small improvements over the pretrained xresnet50 over long enough training cycles, such that the non-trained units could effectively learn, as shown below: . &nbsp; . . Fig 12. Improvement using SOTA model . &nbsp; . Finally, a full 5-Fold Cross-Validation training was undertaken for both the Noisy and Curated set as detailed in Figure 9 in the Implementation section above, with some tweaks to the spectrograms settings, i.e. using top_dB of 60 to ensure only the most prominent Noisy Set features were captured by the mel-spectrograms. This this approach achieved the final score of 0.69788, a marked improvement that would have gained a bronze-medal position in the competition and could certainly be improved upon further. . Up Next . In Part 3 of this blog series, we’ll look at the final results and how these could be improved upon. . If you have any questions or feedback about this post, I’d be very happy to hear them. Please contact me at my GitHub or Twitter using the links below. . References: . Chan,Zhang,Chiu, Zoph,Cubuk,Le - 2019 - SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition &#8617; &#8617;2 &#8617;3 . | https://github.com/lRomul/argus-freesound &#8617; . | https://github.com/ebouteillon/freesound-audio-tagging-2019 &#8617; . | https://medium.com/@mnpinto/multi-label-audio-classification-7th-place-public-lb-solution-for-freesound-audio-tagging-2019-a7ccc0e0a02f &#8617; . |",
            "url": "https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html",
            "relUrl": "/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 1 - Problem Definition, Data Analysis, Methods and Algorithms",
            "content": "Welcome to Part 1 of a blog series based on my Udacity Machine Learning Engineer Nanodegree Capstone project. This initial section deals with the problem definition, outlines the solution approach using the in-development fastai2 audio library and discusses the dataset. . The blog series will be structured as follows: . Problem Definition, Data Analysis, Methods and Algorithms | Methodology and Implementation | Results and Analysis | Links will be provided as the series progresses. Please see the associated GitHub repository for all notebooks. . A huge thanks goes to fastai and the fastai2 audio contributors for their amazing work. . &nbsp; . I. Problem Definition . Overview . The sub-field of Machine Learning known as Machine Listening is a burgeoning area of research using signal processing for the automatic extraction of information from sound by a computational analysis of audio. There are many different areas of research within this field as demonstrated by the latest Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge1, a machine learning challenge dedicated to the research and development of new methods and algorithms for audio. These include: . Acoustic Scene Classification | Sound Event Detection and Localization | Sound Event Detection and Separation in Domestic Environments | Urban Sound tagging | Automated Audio Captioning | . As an acoustic engineer, I am extremely intrigued by this new field. Recent developments in machine learning algorithms have allowed significant progress to be made within this area, with the potential applications of the technology being wide and varied and meaning the tools could prove to be extremely useful for the acoustic practitioner amongst many other uses. . The in-development user-contributed fast.ai2 Audio library2 inspired me to undertake the development of a deep learning audio-tagging system for this Udacity Capstone project, described herein. . Problem Statement . The Freesound Audio Tagging 2019 Kaggle Competition provides the basis for my research project3. . The challenge is to develop a system for the automatic classification of multi-labelled audio files within 80 categories, that could potentially be used for automatic audio/video file tagging with noisy or untagged audio data. This has historically been investigated in a variety of ways: . Conversion of audio to mel-spectrogram images fed into CNNs | End-to-End Deep learning | Custom architectures involving auto-encoders | Features representation transfer learning with custom architectures and Google’s Audioset | . In addition, the classification of weakly labelled data from large-scale crowdsourced datasets provides a further problem for investigation4. The problem is clearly quantifiable in that a number of accuracy metrics could be used to quantify the accuracy of the model’s predictions, described below. . The competition dataset comprises audio clips from the following existing datasets: . “Curated Train Set” - Freesound Dataset (FSD): a smaller dataset collected at the MTG-UPF based on Freesound content organized with the AudioSet Ontology and manually labelled by humans. 4964 files. | “Noisy Train Set” - The soundtracks of a pool of Flickr videos taken from the Yahoo Flickr Creative Commons 100M dataset (YFCC) which are automatically labelled using metadata from the original Flickr clips. These items therefore have significantly more label noise than the Freesound Dataset items. 19815 files. | . The data comprises 80 categories labelled according to Google’s Audioset Ontology 3 with ground truth labels provided at the clip level. The clips range in duration between 0.3 to 30s in uncompressed PCM 16 bit, 44.1kHz mono audio files. . Solution Statement . With the above competition requirements in mind, the proposed solution was followed and was undertaken initially within a Pytorch AWS SageMaker notebook instance using Jupyter Notebooks, and further, using a Google Cloud Platform AI Notebook using fastai2 and fastai2 audio libraries , due to the extra credits required for the long training times on a GPU instance: . The data will be downloaded from Kaggle into the chosen platform AWS SageMaker / GCP AI Notebook instance | Exploratory Data Analysis - The dataset will be downloaded such that the file metadata can be extracted, in order to confirm: sample rates, bit-rates, durations, channels (mono/stereo) for each file in order to direct initial signal processing stage and approach towards the dataset splitting. In addition, the statistics of the file labels will be analysed. | Model Development: The fastai2 and fastai2 audio libraries will be installed | The fastai2 audio library will be used for the data processing, in order to convert the audio files into tensor representations of mel-spectrograms on-the-fly, rather than in a separate pre-processing stage. This is a significant benefit of the library in terms of allowing quick experimentation and iteration within the model development over other methods such as converting all audio files to mel-spectrogram images separately. | In-line with the competition rubric, a non-pretrained convolutional neural network (CNN) using the fastai2 library for PyTorch will be developed using state-of-the-art methods and additions. | The model will be trained on the “Noisy Train” set in a 5-Folds Cross Validation manner, using Sci-Kit Learn’s K-Fold model selection module5. | The results of these 5 models will be used to train on the “Curated Train” set in the same 5-Folds Cross Validation manner as the Curated Train set in order to gain 5 separate models. | Test-Time-Augmentation (TTA) will be used to gain averaged predictions from all 5 final models on the test set. The predictions will be submitted as a Late-Submission for the analysis of the results. | This will be repeated, with tweaks to the model augmentations in order to try to improve the results iteratively. | . | Metrics . Due to the advancement of multi-label audio classification in recent years, a simple multi-label accuracy metric was not used within the Kaggle competition, as performances of the systems can easily exceed 95% within a few epochs of training. . As such, the competition used label-weighted label-ranking average precision (a.k.a lwl-rap) as the evaluation metric. The basis for the metric, the label-ranking average precision algorithm, is described in detail within the Sci-Kit Learn implementation6. The additional adaptations of the metric are to provide the average precision of predicting a ranked list of relevant labels per audio file, which is a significantly more complex problem to solve than a standard multi-label accuracy metric. The overall score is the average over all the labels in the test set, with each label having equal weight (rather than equal weight per test item), as indicated by the “label-weighted” prefix. This is defined as follows7: . &nbsp; . lwlrap=1∑s∣C(s)∣∑a∑eϵ C(s)Prec(s,c)lwlrap = frac{1}{ sum_{s} left | C(s) right |} sum_{a} sum_{e epsilon C(s)}Prec(s,c)lwlrap=∑s​∣C(s)∣1​a∑​eϵ C(s)∑​Prec(s,c) . &nbsp; . where Prec(s,c)Prec(s,c)Prec(s,c) is the label-ranking precision for the list of labels up to class ccc and the set of ground-truth classes for sample sss is C(s)C(s)C(s). ∣C(s)∣ mid C(s) mid∣C(s)∣ is the number of true class labels for sample sss. . The Kaggle competition provides a Google Colab example implementation8. . II. Analysis . Data Exploration . The datasets were downloaded from Kaggle using the Kaggle API and analysed within a Jupyter Notebook. . The first stage of the process was to understand the dataset more fully. Fortunately, due to being a Kaggle Competition dataset it was well documented and clean in terms of organization. . Downloading the dataset was undertaken using guidance given within the Kaggle Forums9 directly into the SageMaker/GCP Instance storage for easy access. . The files were then unzipped for the EDA. For further details, please see the notebook directly. . Pandas and Pandas Profiling . In order to undertake the analysis of the data, the numerical data analysis packages Pandas and Pandas Profiling were used. . Pandas Profiling10 is an extremely useful add-on package to Pandas, which creates HTML profile reports directly from Pandas DataFrames quickly and easily. From the provided .csv files file category labels were analysed and, in addition, the audio file meta-data was extracted (i.e. sample rates, bit-rates, durations, number of channels). . &nbsp; . . Fig 1. An example Pandas DataFrame of extracted audio file info . &nbsp; . Using these two packages the following was found. . Curated Train Data . For the Curated Train dataset, it was found that the bit-rate was a constant 16bits, the channels a constant 1 (mono), constant sample rate of 44.1kHz and that there were 213 different tagging combinations of the 80 audio labels over the total file count (4964 files): . &nbsp; . . Fig 2. Pandas Profiling for the Curated Train Data set . &nbsp; . In terms of the file durations, the average file length was 7.63 seconds and the files ranged between just over 0 and 30 seconds long, with the lengths predominantly in the 0-5 seconds length range. This will affect the mel-spectrogram processing of the data, i.e. we will need to ensure a sufficient amount of both the longer and smaller audio files are taken, in order for the feature learning of the CNN to be accurate. . &nbsp; . . Fig 3. Pandas Profiling information for the audio file durations . &nbsp; . Noisy Train Data . As with the Curated dataset, with the Noisy Train dataset it was found that the bit-rate was a constant 16bits, the channels a constant 1 (mono), constant sample rate of 44.1kHz. However, in this dataset there were 1168 different tagging combinations of the 80 audio labels over the total file count (19815 files): . &nbsp; . . Fig 4. Pandas Profiling for the Noisy Train dataset . &nbsp; . The Noisy Train dataset average file length was significantly longer on average than the Curated set at 14.6s long, however, the files ranged between 1 and 16 seconds long. There is therefore a significant difference in terms of length between the two datasets. . &nbsp; . . Fig 5. Pandas Profiling information for the audio file durations . &nbsp; . In addition, as the name implies, the Noisy Train set files have a significantly higher noise floor than the Curated Train set due to the provenance of the files. . Data Visualisation . The following figure clearly illustrates the differences between the difference in durations of audio files between the two datasets: . &nbsp; . . Fig 6. Train vs Noisy dataset durations (x-axis = seconds) . &nbsp; . Therefore, in the development of the model the following factors will need to be considered: . Noise floor differences between the curated and noisy train set will affect how the signals are clipped to shorter lengths to feed into the CNN. | The average lengths also have a high range of values both over the the individual datasets and between the curated and noisy set, we will need to ensure the main recorded features corresponding to the file labels of each recording are kept within any clipped sections to produce the mel-spectrograms. | . III. Algorithms and Techniques . Mel-Spectrograms . This signal processing stage will involve trimming (to ensure uniform duration) in order be converted to uniform length log-mel-spectrogram representations of the audio. A log-mel-spectrogram is a spectrogram representation of the audio i.e. a frequency-domain representation based on the Fourier Transform (x-axis = time, y axis = frequency and colour depth/pixel value = relative sound intensity) which has been converted to the Mel scale on the y-axis by a non-linear transform in order to be more representative of the highly non-linear magnitude and frequency sensitivities of the human hearing11. The chosen settings will be discussed and shown further in the Data Preprocessing section. . &nbsp; . . Fig 7. Conversion from Waveform to Mel-spectrogram representation . &nbsp; . Convolutional Neural Network (CNN) . The length uniformity of the audio clips in is important, as it allows Rank-2 tensors of the mel-spectrograms to be fed in batches into the CNN. The model variety used was as follows, based on the state of the art findings of the fastai community12 and other research described below. The model and architecture used the following settings: . Architecture: fastai2’s XResNet50 based on the Bag of Tricks13 research paper which includes tweaks to the optimisation methods for higher performance. ResNets use skip connections in order to allow propagation of information more deeply into the architecture, giving significant speed improvements for deeper networks while allowing the gradient descent to backpropagate through the network efficiently which aids in increasing training accuracy. This has further been augmented in the Bag of Tricks paper, whereby the residual block convolutional layers have been re-arranged such that further efficiency gains are made. . | Activation Function: Mish14 which has been shown to provide performance improvements over the standard ReLU activation function due to its smoothing of the activations rather than the cut-off of the ReLU function for values below 0. . | Optimizer Function: Ranger which is a combination of the RAdam15 and Lookahead16 optimizer functions. These functions work as a searching pair, whereby one learner goes ahead of the other to explore the function topography, such that traps involving local minima can be avoided. . | Layer tweaks: Self-Attention Layers17 which allow the network to focus on a subset on the features learned to be most pertinent to the data label. . | Replacing Max Pooling Layers with “MaxBlurPool” layers for better generalization . | Flat-Cosine decay learning rate scheduling . | . K-Folds Validation . Sci-Kit Learn’s KFolds Validation function was used to split the datasets into 5 folds, to allow all of the available data to be used in the training and to further allow the 5 created models to give ensembled predictions on the Test set, which provides a significant performance improvement over a single model. . MixUp . MixUp, whereby two spectrograms are combined to form a third, was also used during the longer Curated Training Set procedure. Detailed further below. . Test-Time Augmentation (TTA) . In addition to the methods outlined above, Test-Time Augmentations were applied to the test set, whereby, during inference, multiple augmented versions of the images were created using the same data transformations as the training procedure and then the average of the maximum of the predictions for each augmented version is used as the final prediction. This provided a further performance boost. . Benchmark . The Baseline performance for the Kaggle Competition was set at 0.53792 which provided a minimum target. The winner18 of the competition achieved 0.75980, which provided the upper target. The details of the winning model and training method can be found on the linked GitHub page, but for brevity, the basic details of the system from the GitHub repo, were as follows: . Log-scaled mel-spectrograms | CNN model with attention, skip connections and auxiliary classifiers | SpecAugment, Mixup augmentations | Hand relabeling of the curated dataset samples with a low score | Ensembling with an MLP second-level model and a geometric mean blending | . Up Next . In Part 2 of this blog series, we will look at the methodology and implementation of training the model and improving it iteratively. . If you have any questions or feedback about this post, I’d be very happy to hear them. Please contact me at my GitHub or Twitter using the links below. . References: . http://dcase.community/challenge2020/index &#8617; . | https://github.com/rbracco/fastai2_audio &#8617; . | https://www.kaggle.com/c/freesound-audio-tagging-2019/overview &#8617; &#8617;2 . | Learning Sound Event Classifiers from Web Audio with Noisy Labels - Fonseca et al. 2019 &#8617; . | https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html &#8617; . | https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision &#8617; . | Fonseca et al. - Audio tagging with noisy labels and minimal supervision. In Proceedings of DCASE2019 Workshop, NYC, US (2019). &#8617; . | https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8 &#8617; . | https://www.kaggle.com/c/deepfake-detection-challenge/discussion/129521 &#8617; . | https://github.com/pandas-profiling/pandas-profiling &#8617; . | Computational Analysis of Sound Scenes and Events, pg. 22 - Virtanen et al. &#8617; . | https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/04_ImageWoof.ipynb &#8617; . | He, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. “Bag of Tricks for Image Classification with Convolutional Neural Networks.” CoRR abs/1812.01187 &#8617; . | Misra, Diganta. 2019. “Mish: A Self Regularized Non-Monotonic Neural Activation Function.” &#8617; . | Liyuan Liu et al. 2019 - On the Variance of the Adaptive Learning rate and Beyond &#8617; . | Zhang, Lucas Hinton, Ba - Lookahead Optimizer: k steps forward, 1 step back &#8617; . | Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena 2018 - Self-Attention Generative Adversarial Networks &#8617; . | https://github.com/lRomul/argus-freesound &#8617; . |",
            "url": "https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/05/Udacity-Capstone-Part-1-Definition-and-Data.html",
            "relUrl": "/deep%20learning/fastai2/audio/markdown/2020/06/05/Udacity-Capstone-Part-1-Definition-and-Data.html",
            "date": " • Jun 5, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Mike Fuller, an acoustic engineer who is currently an MSc Artificial Intelligence candidates at the University of Bath, UK. Welcome to my blog about my journey into AI and machine learning. . If you have any queries on any of the posts, or just want to get in touch, please feel free to send me a message at the links provided. .",
          "url": "https://mikful.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mikful.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}