<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 2 - Methodology and Implementation | Mike’s Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 2 - Methodology and Implementation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Udacity Machine Learning Engineer Nanodegree Capstone Project" />
<meta property="og:description" content="A Udacity Machine Learning Engineer Nanodegree Capstone Project" />
<link rel="canonical" href="https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html" />
<meta property="og:url" content="https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html" />
<meta property="og:site_name" content="Mike’s Blog" />
<meta property="og:image" content="https://mikful.github.io/blog/images/udacity-capstone-series/melspec4-part2.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-12T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"A Udacity Machine Learning Engineer Nanodegree Capstone Project","headline":"Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 2 - Methodology and Implementation","dateModified":"2020-06-12T00:00:00-05:00","datePublished":"2020-06-12T00:00:00-05:00","@type":"BlogPosting","image":"https://mikful.github.io/blog/images/udacity-capstone-series/melspec4-part2.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html"},"url":"https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mikful.github.io/blog/feed.xml" title="Mike's Blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 2 - Methodology and Implementation | Mike’s Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 2 - Methodology and Implementation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Udacity Machine Learning Engineer Nanodegree Capstone Project" />
<meta property="og:description" content="A Udacity Machine Learning Engineer Nanodegree Capstone Project" />
<link rel="canonical" href="https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html" />
<meta property="og:url" content="https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html" />
<meta property="og:site_name" content="Mike’s Blog" />
<meta property="og:image" content="https://mikful.github.io/blog/images/udacity-capstone-series/melspec4-part2.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-12T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"A Udacity Machine Learning Engineer Nanodegree Capstone Project","headline":"Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 2 - Methodology and Implementation","dateModified":"2020-06-12T00:00:00-05:00","datePublished":"2020-06-12T00:00:00-05:00","@type":"BlogPosting","image":"https://mikful.github.io/blog/images/udacity-capstone-series/melspec4-part2.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html"},"url":"https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://mikful.github.io/blog/feed.xml" title="Mike's Blog" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Mike&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Multi-Label Auto-Tagging of Noisy Audio Using fastai2 - Part 2 - Methodology and Implementation</h1><p class="page-description">A Udacity Machine Learning Engineer Nanodegree Capstone Project</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-12T00:00:00-05:00" itemprop="datePublished">
        Jun 12, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#deep learning">deep learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#fastai2">fastai2</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#audio">audio</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Welcome to Part 2 of a blog series based on my Udacity Machine Learning Engineer Nanodegree Capstone project.  This section defines the model implementation using the in-development fastai2 audio library and Google Cloud AI Platform notebooks.</p>

<p>The blog series is structured as follows, please follow the links for other sections:</p>

<ol>
  <li><a href="https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/05/Udacity-Capstone-Part-1-Definition-and-Data.html">Problem Definition, Analysis, Methods and Algorithms</a></li>
  <li>Methodology and Implementation</li>
  <li><a href="https://mikful.github.io/blog/deep%20learning/fastai2/audio/markdown/2020/06/19/Udacity-Capstone-Part-3-Results-and-Analysis.html">Results and Analysis</a></li>
</ol>

<p>Links will be provided as the series progresses. Please see <a href="https://github.com/mikful/udacity-mlend-capstone">the associated GitHub repository</a> for all notebooks.</p>

<p>A huge thanks goes to fastai and the fastai2 audio contributors for their amazing work.</p>

<p>&nbsp;</p>

<h2 id="iv-methodology-and-implementation">IV. Methodology and Implementation</h2>
<h3 id="data-pre-processing">Data Pre-processing</h3>

<p><strong>Fastai2 Audio</strong></p>

<p>The remarkable in-development fastai2 audio package was used to convert the audio files into mel-spectrogram 2D tensors on-the-fly, as a form of highly efficient data processing, rather than pre-processing and having to save spectrograms to a different dataset. This was done using the following process:</p>

<ol>
  <li>Create Pandas DataFrames for the files, suing the <code class="highlighter-rouge">train_curated.csv</code> and <code class="highlighter-rouge">train_noisy.csv</code> files provided by the competition, removing corrupted or empty files as given in the competition guidance:</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_train_curated_df</span><span class="p">(</span><span class="nb">file</span><span class="p">,</span> <span class="n">remove_files</span><span class="o">=</span><span class="p">[]):</span>
    <span class="n">df_curated</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
    <span class="n">df_curated</span><span class="p">[</span><span class="s">'fname'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'../data/train_curated/'</span> <span class="o">+</span> 	<span class="n">df_curated</span><span class="p">[</span><span class="s">'fname'</span><span class="p">]</span> 
    <span class="n">df_curated</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'fname'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">df_curated</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">remove_files</span><span class="p">]</span>
    <span class="n">df_curated</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">remove_files</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">df_curated</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df_curated</span>

<span class="k">def</span> <span class="nf">create_train_noisy_df</span><span class="p">(</span><span class="nb">file</span><span class="p">):</span>
    <span class="n">df_noisy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
    <span class="n">df_noisy</span><span class="p">[</span><span class="s">'fname'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'../data/train_noisy/'</span> <span class="o">+</span> <span class="n">df_noisy</span><span class="p">[</span><span class="s">'fname'</span><span class="p">]</span> 
    <span class="k">return</span> <span class="n">df_noisy</span>


<span class="c1"># Create Curated training set df
# Remove corrupt and empty files as per Kaggle guidance
</span>
<span class="n">remove_files</span> <span class="o">=</span> <span class="p">[</span><span class="s">'f76181c4.wav'</span><span class="p">,</span> <span class="s">'77b925c2.wav'</span><span class="p">,</span> <span class="s">'6a1f682a.wav'</span><span class="p">,</span> <span class="s">'c7db12aa.wav'</span><span class="p">,</span> <span class="s">'7752cc8a.wav'</span><span class="p">,</span> <span class="s">'1d44b0bd.wav'</span><span class="p">]</span>
<span class="n">remove_files</span> <span class="o">=</span> <span class="p">[</span><span class="s">'../data/train_curated/'</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">remove_files</span><span class="p">]</span>
<span class="n">df_curated</span> <span class="o">=</span> <span class="n">create_train_curated_df</span><span class="p">(</span><span class="s">'../data/train_curated.csv'</span><span class="p">,</span> <span class="n">remove_files</span><span class="o">=</span><span class="n">remove_files</span><span class="p">)</span>
<span class="n">df_curated</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<p>The DataFrames were then used to supply the fastai DataBlock API with the filenames, which could then be processed using the fastai2 audio <code class="highlighter-rouge">item_transformations</code> which are applied to each file before training. After significant testing iterations the audio transformation settings were chosen as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DBMelSpec</span> <span class="o">=</span> <span class="n">SpectrogramTransformer</span><span class="p">(</span><span class="n">mel</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">to_db</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># convert to Mel-spectrograms
</span>
<span class="n">clip_length</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># clip subsection length in seconds
</span><span class="n">sr</span> <span class="o">=</span> <span class="mi">44100</span> <span class="c1"># sample rate
</span><span class="n">f_min</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1"># mel-spectrogram minimum frequency
</span><span class="n">f_max</span> <span class="o">=</span> <span class="mi">20000</span> <span class="c1"># mel-spectrogram minimum frequency
</span><span class="n">n_mels</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1"># mel-frequency bins, dictates the y-axis pixel size
</span><span class="n">hop_length</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">clip_length</span><span class="o">*</span><span class="n">sr</span><span class="p">)</span><span class="o">/</span><span class="n">n_mels</span><span class="p">)</span><span class="c1"># determines width of image. for square to match n_mels, set math.ceil((clip_length*sr)/n_mels)
</span><span class="n">nfft</span> <span class="o">=</span> <span class="n">n_mels</span> <span class="o">*</span> <span class="mi">20</span> <span class="c1"># = 2560 for higher resolution in y-axis
</span><span class="n">win_length</span> <span class="o">=</span> <span class="mi">1024</span> <span class="c1"># sample windowing
</span><span class="n">top_db</span> <span class="o">=</span> <span class="mi">60</span> <span class="c1"># highest noise level in relative db
</span></code></pre></div></div>

<ul>
  <li>The <code class="highlighter-rouge">top_db</code> parameter setting of 60dB was important, as the noisy train set had high background noise (low signal-to-noise ratio) which with a higher setting lead to obscured features in the mel-spectrograms.</li>
</ul>

<p>In addition to the mel-spectrogram settings above, the following additional item transformations were undertaken:</p>

<ul>
  <li>
    <p><code class="highlighter-rouge">RemoveSilence</code>  - Splits the original signal at points of silence more than <code class="highlighter-rouge">2 * pad_ms</code></p>
  </li>
  <li>
    <p><code class="highlighter-rouge">CropSignal</code> - Crops a signal by <code class="highlighter-rouge">clip_length</code> seconds and adds zero-padding by default if the signal is less than <code class="highlighter-rouge">clip_length</code></p>
  </li>
  <li>
    <p><code class="highlighter-rouge">aud2spec</code> - The mel-spectrogram settings from above</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">MaskTime</code> - Uses Google’s SpecAugment<sup id="fnref:20" role="doc-noteref"><a href="#fn:20" class="footnote">1</a></sup> time masking procedure to zero-out time domain information as a form of data augmentation</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">MaskFreq</code> - Uses Google’s SpecAugment<sup id="fnref:20:1" role="doc-noteref"><a href="#fn:20" class="footnote">1</a></sup> frequency masking procedure to zero-out frequency domain information as a form of data augmentation</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">item_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveSilence</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span>  
             <span class="n">CropSignal</span><span class="p">(</span><span class="n">clip_length</span><span class="o">*</span><span class="mi">1000</span><span class="p">),</span>  
             <span class="n">aud2spec</span><span class="p">,</span>
             <span class="n">MaskTime</span><span class="p">(</span><span class="n">num_masks</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span> <span class="n">MaskFreq</span><span class="p">(</span><span class="n">num_masks</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">)]</span>
</code></pre></div></div>

<p><strong>Batch Transforms</strong></p>

<p>In addition to the item transforms above, Batch Transforms were used as part of the DataBlock API, which are transformations applied per batch during training:</p>

<ul>
  <li><code class="highlighter-rouge">Normalize()</code> - normalizes the data taking a single batch’s statistics</li>
  <li><code class="highlighter-rouge">RatioResize(256)</code> - during training (other than the first 10 epochs of the noisy data for speed), the mel-spectrogram tensors were resized from 128x128px to 256x256px through bilinear interpolation as this has been shown to give gains in performance over simply creating a 256x256 tensor from the outset.</li>
  <li><code class="highlighter-rouge">Brightness and Contrast</code> augmentations were also applied in the training cycles to improve performance</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">Normalize</span><span class="p">(),</span>
              <span class="n">RatioResize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
              <span class="n">Brightness</span><span class="p">(</span><span class="n">max_lighting</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.75</span><span class="p">),</span>
              <span class="n">Contrast</span><span class="p">(</span><span class="n">max_lighting</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)]</span>
</code></pre></div></div>

<p>No further augmentations were applied as would otherwise be typical in many image classification processes. Many typical image augmentations applied  are not suitable for spectrogram representations of audio, for example, cropping/warping/rotating the spectrogram tensor would warp the relative frequency relationship and thus, would not gain any benefit at testing time. This was found to be true during tests of various transforms.</p>

<p>The above augmentations (prior to batch transformations), produced the following mel-spectrograms as 2D tensors (note the time and frequency masking augmentations):</p>

<p>&nbsp;</p>
<p><img src="/blog/images/udacity-capstone-series/aug-mel-spectrograms.jpg" alt="Augmented Mel Spectrograms" /></p>
<p style="text-align: center;">Fig 8. Augmented Mel-spectrograms</p>
<p>&nbsp;</p>

<h3 id="implementation">Implementation</h3>

<p>The data augmentations stated above were used to significantly improve the performance of the classifier during the following K-Fold training cycles.</p>

<p>The implemented training method was chosen based on the Competitions 6th place winner’s technique<sup id="fnref:20:2" role="doc-noteref"><a href="#fn:20" class="footnote">1</a></sup>, however, only the first two stages were implemented as follows due to the cost requirements using GCP:</p>

<p>&nbsp;</p>
<p><img src="/blog/images/udacity-capstone-series/train-test-method.jpg" alt="train-test-method" /></p>
<p style="text-align: center;">Fig 9. Train-Test-Prediction Stages</p>
<p>&nbsp;</p>

<p><strong>Stage 1 - Noisy Training Set</strong></p>

<p>As can be seen below, the following 5-Fold training cycle was used on the noisy set. The indices of the DataFrame were shuffled to ensure the data splits were chosen at random, but without overlap using SKLearn’s k-Folds module. The cycle began with 10 epochs of training at a higher learning rate and then 10 epochs of training at a lower learning rate (set after using fastai’s learning rate finder during the testing stage) used to fine-tune the model’s weights further. Please see the associated Jupyter Notebook for the training output.</p>

<p>The models were then saved for further training on the curated training set.</p>

<p><em>Note: No MixUp augmentations were used on the Noisy Training set.</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="c1"># Declare Number Folds 
</span><span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># random_state for repeatable results, shuffle indices
</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df_noisy</span> <span class="c1"># to use random subset, use  df = df_.sample(frac=0.5, replace=False, random_state=1) # take random subset of the noisy dataframe for faster training (otherwise need 6.5 hours for all folds with complete dataset)
</span>
<span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">valid_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'</span><span class="se">\n</span><span class="s">Noisy Train Set - Fold {fold+1}/{n_splits}'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_x</span><span class="p">(</span><span class="n">r</span><span class="p">):</span> <span class="k">return</span> <span class="n">r</span><span class="p">[</span><span class="s">'fname'</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">get_y</span><span class="p">(</span><span class="n">r</span><span class="p">):</span> <span class="k">return</span> <span class="n">r</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">','</span><span class="p">)</span> <span class="c1"># split labels on ','
</span>        
    <span class="k">def</span> <span class="nf">get_dls</span><span class="p">(</span><span class="n">train_cycle</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">train_cycle</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">Normalize</span><span class="p">(),</span>
                          <span class="n">Brightness</span><span class="p">(</span><span class="n">max_lighting</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.75</span><span class="p">),</span>
                          <span class="n">Contrast</span><span class="p">(</span><span class="n">max_lighting</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)]</span>
            
        <span class="k">elif</span> <span class="n">train_cycle</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">Normalize</span><span class="p">(),</span>
                          <span class="n">RatioResize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span> <span class="c1"># progressive resize to 256x256px
</span>                          <span class="n">Brightness</span><span class="p">(</span><span class="n">max_lighting</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.75</span><span class="p">),</span>
                          <span class="n">Contrast</span><span class="p">(</span><span class="n">max_lighting</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)]</span>
        
        <span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">AudioBlock</span><span class="p">,</span> <span class="n">MultiCategoryBlock</span><span class="p">),</span>
                           <span class="n">splitter</span><span class="o">=</span><span class="n">IndexSplitter</span><span class="p">(</span><span class="n">valid_idx</span><span class="p">),</span> <span class="c1"># split using df index
</span>                           <span class="n">get_x</span><span class="o">=</span><span class="n">get_x</span><span class="p">,</span>
                           <span class="n">get_y</span><span class="o">=</span><span class="n">get_y</span><span class="p">,</span>
                           <span class="n">item_tfms</span> <span class="o">=</span> <span class="n">item_tfms</span><span class="p">,</span>
                           <span class="n">batch_tfms</span> <span class="o">=</span> <span class="n">batch_tfms</span>
                          <span class="p">)</span>
        <span class="k">return</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
    
    <span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="n">train_cycle</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">xresnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">act_cls</span><span class="o">=</span><span class="n">Mish</span><span class="p">,</span> <span class="n">sa</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span> <span class="c1">#create custom xresnet: 1 input channel,  80 output nodes, self-attention, Mish activation function
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">convert_MP_to_blurMP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">)</span> <span class="c1"># convert MaxPool2D layers to MaxBlurPool
</span>    <span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">BCEWithLogitsLossFlat</span><span class="p">(),</span> <span class="n">opt_func</span> <span class="o">=</span> <span class="n">ranger</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">lwlrap</span><span class="p">])</span> <span class="c1"># pass custom model to Learner, no mixup for noisy set as fewer epochs
</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">'Batch transforming images to 256x256px and training further.'</span><span class="p">)</span>
    
    <span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="n">train_cycle</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">dls</span> <span class="o">=</span> <span class="n">dls</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">'Saving Learner...'</span><span class="p">)</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="s">'stage-1_noisy_fold-{fold+1}_sota2'</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Stage 2 - Curated Train Set</strong></p>

<p>After all 5 models had been trained on the Noisy set, the models were then trained on different 5-folds of the Curated Set. This essentially gave 5 distinct models, all trained on different data for later ensembling.</p>

<p><em>Note: MixUp data augmentations were applied to the Curated Train set, shown as training callback below. This is whereby two spectrogram tensors are combined into a single 2D tensor with a certain percentage blend (50% in this case), allowing the network to learn double the amount of features and labels per batch. This also provides a form of regularization for the model which improves generalization on the validation/test sets.</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## K-Folds training loop
</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df_curated</span>

<span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">valid_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'</span><span class="se">\n</span><span class="s">Curated Train Set - Fold {fold+1}/{n_splits}'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_x</span><span class="p">(</span><span class="n">r</span><span class="p">):</span> <span class="k">return</span> <span class="n">r</span><span class="p">[</span><span class="s">'fname'</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">get_y</span><span class="p">(</span><span class="n">r</span><span class="p">):</span> <span class="k">return</span> <span class="n">r</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">','</span><span class="p">)</span> <span class="c1"># split labels on ','
</span>
    <span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">AudioBlock</span><span class="p">,</span> <span class="n">MultiCategoryBlock</span><span class="p">),</span>
                       <span class="n">splitter</span><span class="o">=</span><span class="n">IndexSplitter</span><span class="p">(</span><span class="n">valid_idx</span><span class="p">),</span> <span class="c1"># split using df index
</span>                       <span class="n">get_x</span><span class="o">=</span><span class="n">get_x</span><span class="p">,</span>
                       <span class="n">get_y</span><span class="o">=</span><span class="n">get_y</span><span class="p">,</span>
                       <span class="n">item_tfms</span> <span class="o">=</span> <span class="n">item_tfms</span><span class="p">,</span>
                       <span class="n">batch_tfms</span> <span class="o">=</span> <span class="n">batch_tfms</span> <span class="c1"># including RatioResize(256)
</span>                      <span class="p">)</span>

    <span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

    <span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'</span><span class="se">\n</span><span class="s">Loading Stage 1 model - fold {fold+1}.'</span><span class="p">)</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">xresnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">act_cls</span><span class="o">=</span><span class="n">Mish</span><span class="p">,</span> <span class="n">sa</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span> <span class="c1">#create custom xresnet: 1 input channel,  80 output nodes, self-attention, Mish activation function
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">convert_MP_to_blurMP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">)</span> <span class="c1"># convert MaxPool2D layers to MaxBlurPool
</span>    <span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">BCEWithLogitsLossFlat</span><span class="p">(),</span>  <span class="n">opt_func</span><span class="o">=</span><span class="n">ranger</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">lwlrap</span><span class="p">])</span> <span class="c1"># pass custom model to Learner, no mixup for noisy set as fewer epochs
</span>    <span class="n">learn</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="s">'stage-1_noisy_fold-{fold+1}_sota2'</span><span class="p">)</span>
    
    <span class="n">learn</span><span class="o">.</span><span class="n">dls</span> <span class="o">=</span> <span class="n">dls</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">add_cb</span><span class="p">(</span><span class="n">MixUp</span><span class="p">())</span> <span class="c1"># add mixup callback
</span>    
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Training on Curated Set:'</span><span class="p">)</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mf">3e-4</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">'Saving model...'</span><span class="p">)</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="s">'stage-2_curated_fold-{fold+1}_sota2'</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Testing</strong></p>

<p>At the testing stage, Test-Time-Augmentations and ensembling the predictions of all 5 different Stage-2 models were used to improve the final predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># grab test filenames from submission csv
</span><span class="n">df_fnames</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'../data/sample_submission.csv'</span><span class="p">)</span>
<span class="n">fnames</span> <span class="o">=</span> <span class="n">df_fnames</span><span class="o">.</span><span class="n">fname</span>
<span class="n">df_fnames</span> <span class="o">=</span> <span class="s">'../data/test/'</span> <span class="o">+</span> <span class="n">df_fnames</span><span class="o">.</span><span class="n">fname</span>
<span class="k">print</span><span class="p">(</span><span class="n">df_fnames</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="c1"># get predictions
</span><span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
    <span class="n">stage</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Getting predictions from stage {stage} fold {fold+1} model.'</span><span class="p">)</span>

    <span class="n">learn</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="s">'stage-2_curated_fold-{fold+1}_sota2'</span><span class="p">)</span>

    <span class="n">dl</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">dls</span><span class="o">.</span><span class="n">test_dl</span><span class="p">(</span><span class="n">df_fnames</span><span class="p">)</span>
    
    <span class="c1"># predict using tta    
</span>    <span class="n">preds</span><span class="p">,</span> <span class="n">targs</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">tta</span><span class="p">(</span><span class="n">dl</span><span class="o">=</span><span class="n">dl</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">fold</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">preds</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">+=</span> <span class="n">preds</span>


<span class="c1"># Average predictions
</span><span class="n">predictions</span> <span class="o">/=</span> <span class="n">n_splits</span>

<span class="c1"># Create Submission DataFrame    
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">df_sub</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">dls</span><span class="o">.</span><span class="n">vocab</span>
<span class="n">df_sub</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s">"fname"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">df_sub</span><span class="o">.</span><span class="n">fname</span> <span class="o">=</span> <span class="n">fnames</span>
<span class="n">df_sub</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<p>The produced .csv file was then submitted to Kaggle.</p>

<h3 id="refinement">Refinement</h3>

<p><strong>Initial Model</strong></p>

<p>The  initial CNN architecture used was a pre-trained (on ImageNet) xresnet50 model for speed of iteration. This was trained on a single fold smaller subset of the Noisy data (ranging from 20-80% using the DataBlock API - <code class="highlighter-rouge">RandomSubsetSplitter()</code>function) used for faster iteration on the noisy subset, while all of the Curated data was used. The data augmentation settings were slightly different however, as non-square mel-spectrograms were used to see if larger spectrograms could give improved scores, which was the case, however, this was at the expense of training time.</p>

<p>This highest score achieved by any initial model, was an lwl-rap of 0.61013 on the test-set:</p>

<p>&nbsp;</p>
<p><img src="/blog/images/udacity-capstone-series/image-20200418112147418.png" alt="Initial Best Score" /></p>
<p style="text-align: center;">Fig 10. Initial Best Score</p>
<p>&nbsp;</p>

<p>This score, while not bad for a small amount of testing and still beating the competition baseline, was far from achieving near state-of-the-art performance.</p>

<p>Test-Time-Augmentation was shown to provide a benefit of &gt;3% improvement during further testing rounds using a single training fold on the noisy and curated datasets, shown as the top score in the following image:</p>

<p>&nbsp;</p>
<p><img src="/blog/images/udacity-capstone-series/image-20200418113646639.png" alt="Improvement using TTA" /></p>
<p style="text-align: center;">Fig 11. Improvement using TTA</p>
<p>&nbsp;</p>

<p>After reading further the writeups of the competition winners and high scorers <sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote">2</a></sup><sup id="fnref:21" role="doc-noteref"><a href="#fn:21" class="footnote">3</a></sup><sup id="fnref:22" role="doc-noteref"><a href="#fn:22" class="footnote">4</a></sup>, it was decided that a K-Folds validation approach was required in order to substantially improve the performance.</p>

<p>In addition, due to the large size of the spectrograms in the initial testing phase that would cause extremely slow training over so many folds (a 5x increase in epochs), these were replaced with smaller 128x128px (using 128 mel-bins and the settings shown in the above Data Pre-processing section). It was decided to first try training for a single fold on the Noisy set (90%/10% train/test split) and then split this model into 5 separate models for the further training on the Curated Set.</p>

<p>What’s more, the pretrained xresnet50 model was replaced by the state-of-the-art xresnet50 model described in the previous Section II: Algorithms and Techniques. This was in line with the allowance of only non-pretrained models in the competition and was also shown to provide small improvements over the pretrained xresnet50 over long enough training cycles, such that the non-trained units could effectively learn, as shown below:</p>

<p>&nbsp;</p>
<p><img src="/blog/images/udacity-capstone-series/image-20200418113528769.png" alt="Improvement with SOTA model" /></p>
<p style="text-align: center;">Fig 12. Improvement using SOTA model</p>
<p>&nbsp;</p>

<p>Finally, a full 5-Fold Cross-Validation training was undertaken for both the Noisy and Curated set as detailed in Figure 9 in the Implementation section above, with some tweaks to the spectrograms settings, i.e. using <code class="highlighter-rouge">top_dB</code> of 60 to ensure only the most prominent Noisy Set features were captured by the mel-spectrograms. This this approach achieved the final score of 0.69788, a marked improvement that would have gained a bronze-medal position in the competition and could certainly be improved upon further.</p>

<h2 id="up-next">Up Next</h2>

<p>In Part 3 of this blog series, we’ll look at the final results and how these could be improved upon.</p>

<p>If you have any questions or feedback about this post, I’d be very happy to hear them. Please contact me at my GitHub or Twitter using the links below.</p>

<h2 id="references">References:</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:20" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1904.08779">Chan,Zhang,Chiu, Zoph,Cubuk,Le - 2019 - SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</a> <a href="#fnref:20" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:20:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:20:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:19" role="doc-endnote">
      <p><a href="https://github.com/lRomul/argus-freesound">https://github.com/lRomul/argus-freesound</a> <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:21" role="doc-endnote">
      <p><a href="https://github.com/ebouteillon/freesound-audio-tagging-2019">https://github.com/ebouteillon/freesound-audio-tagging-2019</a> <a href="#fnref:21" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:22" role="doc-endnote">
      <p><a href="https://medium.com/@mnpinto/multi-label-audio-classification-7th-place-public-lb-solution-for-freesound-audio-tagging-2019-a7ccc0e0a02f">https://medium.com/@mnpinto/multi-label-audio-classification-7th-place-public-lb-solution-for-freesound-audio-tagging-2019-a7ccc0e0a02f</a> <a href="#fnref:22" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/blog/deep%20learning/fastai2/audio/markdown/2020/06/12/Udacity-Capstone-Part-2-Implementation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog about my journey into machine learning and AI</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/mikful" title="mikful"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/mikful" title="mikful"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
